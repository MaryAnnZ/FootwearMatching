% Copyright (C) 2014-2017 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{appendix}
\usepackage{rotating}
\usepackage{float}
\usepackage{afterpage}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Rebeka Koszticsak} % The author name without titles.
\newcommand{\thesistitle}{Enhancement of Footwear Impressions} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{Bsc}{male}
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Robert  Sablatnig}{}{male}

% For bachelor and master theses:
\setfirstassistant{Associate Prof.}{Hideki Nakayama}{Ph.D.}{male}
\setsecondassistant{Projektass. Dipl.-Ing.}{Manuel  Keglevic}{}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setaddress{Address}
\setregnumber{01325492}
\setdate{04}{01}{2020} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Enhancement of Footwear Impressions} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Visual Computing}{Visual Computing} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to
%  http://www.informatik.tuwien.ac.at/dekanat

\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
\addtitlepage{english} % English title page.
\addstatementpage

\begin{danksagung*}
\par
Ich möchte mich bei Professor Sablatnig und bei Professor Nakayama bedanken, weil Sie mir erlaubt haben auf der University of Tokyo zu studieren, und Sie mein Projekt unterstützt haben.
\par
Ich möchte mich bei Eszter Takács für das Korrekturlesen bedanken, und weil sie in der schlimmsten Zeit bei mir gestanden ist.
Außerdem möchte ich mich bei ihr und bei Peter Pollak bedanken, weil sie meine Stimmungsschwankungen ausgehalten haben.
\par
Végül, a szüleimnek szeretném megköszönni, hogy megtanították, hogy mi az, ami igazán számít.
\end{danksagung*}

\begin{acknowledgements*}
\par
I would like to thank Professor Sablatnig and Professor  Nakayama for allowing me to study at the University of Tokyo and for supporting my project.
\par
I would like to thank Eszter Takács for proofreading and for holding my hands in the worst times.
Furthermore, I would like to thank her and Peter Pollak for taking my mood swings during the last semester.
\par
Finally, I want to thank my parents for teaching me what really matters.
\end{acknowledgements*}

\begin{kurzfassung}
\par
Obwohl die Erkennung von Schuhabdrücke von Tatorten ein hoch erforschtes Thema ist, erfolgt die endgültige Identifizierung  durch forensische Experten.
Ausserdem ist die Forschung über Preprocessing und Enhancement Techniken um die Matching Genauigkeit zu erhöhen limitiert.
\par
Diese Arbeit untersucht Möglichkeiten zur Verbesserung von Schuhabdrucksbilder aus einem Datensatz mit forensischen Bildern aus der Praxis. 
Die Methodik von drei Ansätzen werden vorgestellt und bewertet. 
Eine Herausforderung bei automatischer Analyse ist das Muster der Schuhsole unabhängig von dem vielseitigen, möglicherweise stark strukturierten und überfüllten Rauschen, korrekt zu filtern.
Neben vollautomatisierten Methoden wird auch eine halbautomatische Technik getestet, bei der Benutzereingaben für die Entfernung von Rauschen und Verzerrungen erforderlich sind. 
\par
Ziel dieser Arbeit ist es, einen State- of-the-Art Überblick zu geben und einen Algorithmus zu definieren, mit dem die Schuhabdrucksbilder trotz Rauschen und variierender Bildqualität gefiltert und verbessert werden können.
Zusammen mit zwei vollautomatischen Algorithmen wird eine halbautomatische Methode zur Rauschunterdrückung und Verbesserung der Schuhabdruckbilder vorgeschlagen. 
Die halbautomatische Methodik in der Arbeit identifiziert verrauschte Pixel basierend auf den Fourier-Mellin-Merkmalen eines von dem Benutzer ausgewählten Hintergrundpixels und dessen Nachbarschaft. 
Gleichzeitig wird ein Rauschmodell berechnet, um diese Struktur auch von dem restlichen Bild, also von dem Schuhabdruckmuster, zu eliminieren. 
Zusätzlich werden die Pixel basierend auf ihren Gradienten gruppiert und der Ansatz von "non-local means" wird angewendet um den Kontrast zwischen dem Vor- und dem Hintergrund zu erhöhen.
Die experimentellen Ergebnisse zeigen, dass die verarbeiteten Bilder klarer sind, das Muster von der Schusohle schärfer ist und das Rauschen unterdrückt wird. 
Neben der qualitativen Bewertung werden unsere Ergebnisse auch anhand einer quantitativen Analyse bewertet und unter Verwendung von drei grundlegenden Bilddeskriptoren durchgeführt.
Die Evaluierung zeigt, dass die verbesserten Bilder eine höhere Genauigkeit erzielt als die Originalbilder.

\end{kurzfassung}



\begin{abstract}
\par
Even though the recognition of shoeprint images secured at crime scenes is a highly researched topic, the final identification is usually done by human forensic experts.
Furthermore, there is limited research available about preprocessing and enhancement techniques, to increase the matching accuracy.
\par
This thesis investigates the possibilities for enhancement of shoeprint images from a real-life dataset, for which three prototypical approaches are presented and evaluated.
The main challenge of this task is to correctly filter the shoe outsole pattern on the shoeprint image regardless of the versatile, possibly heavily structured and cluttered noise in the images.
Among fully automated methods, a semi-automated technique is also tested, where user input is required for noise separation.
\par
The aim of this work is to give an overview of the current state of the art and to define an approach which is able to filter and enhance the shoeprint despite the presence of noise and varying image quality.
Along two fully-automated algorithms a semi-automated noise-suppression and enhancement pipeline for shoeprint images is introduced. 
The noisy pixels are identified based on the Fourier-Mellin features of a background pixel and its neighborhood selected by the user.
At the same time a noise model is calculated to eliminate that structure on the remaining parts of the shoeprint image, thus on the shoeprint, as well.
Additionally, the pixels are clustered based on their gradients and the non-local means approach is applied.
The experimental results show that the processed images are clearer, the shoeprint is sharper and the noise is reduced.
Furthermore besides a qualitative evaluation also a quantitative evaluation is performed using three basic image descriptor features.
These results show that the enhanced images achieve higher accuracy than the original images.


\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}
\raggedbottom
\par
Shoeprints found at crime scenes are important clues or evidences during criminal investigation \cite{kong2014novel}.
Even though at one third of the crime scenes usable shoeprint images are secured \cite{alexandre1996computerized}, human input is required to identify and analyze the shoeprint images  found \cite{wang2014automatic}.
The work of forensic experts is not only time consuming and expensive, but there is no guarantee about the objectivity of the final outcome\cite{gueham2008automatic}.
Furthermore, the stages of the human matching process are not necessarily reproducible considering the opinions of multiple experts \cite{damary2018dependence}.
\par
There is an excessive amount of research already done in order to help or replace the work of forensic experts \cite{rida2019forensic}.
However this task is challenging because of the versatility of conditions: the features and properties of the shoe outsole, such as age, material, etc., the characteristics of the ground where the shoeprint is left and environmental conditions, for example weather, highly influence the overall appearance of the acquired sample \cite{kortylewski2014unsupervised}.
Figure \ref{fig:int:varying} shows images of the same shoe outsole captured under different conditions from the FID-300 \cite{kortylewski2014unsupervised} dataset.
The factors, mentioned above, result in changing appearance causing high intra-class variance while searching for common features of the same shoe outsole pattern.
To make the features robust and to lower the influence of the discussed factors shoeprint enhancement is proposed.
\par
In 2014 a dataset called FID-300 \cite{kortylewski2014unsupervised} was released that consists of shoeprint samples collected by the police. 
Additionally, it contains over 1000 reference shoeprints acquired in controlled conditions.
Moreover, the database introduces 300 new real-life shoeprint samples providing an insight to images forensic experts work with on a daily basis.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00003.jpg}
	\caption{}
	\label{fig:int:varying:3}
  \end{subfigure}
   \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00009.jpg}
	\caption{}
	\label{fig:int:varying:9}
  \end{subfigure}
 \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00017.jpg}
	\caption{}
	\label{fig:int:varying:17}
  \end{subfigure}
 \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00020.jpg}
	\caption{}
	\label{fig:int:varying:20}
  \end{subfigure}
 \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00021.jpg}
	\caption{}
	\label{fig:int:varying:21}
  \end{subfigure}
 \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00025.jpg}
	\caption{}
	\label{fig:int:varying:25}
  \end{subfigure}
 \begin{subfigure}[t]{0.13\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00066.jpg}
	\caption{}
	\label{fig:int:varying:66}
  \end{subfigure}
  \caption{Example images from the FID-300 \cite{kortylewski2014unsupervised} dataset, where the soheprint is captured under different conditions.}
  \label{fig:int:varying}
\end{figure}

\section{Problem Definition}
\par
This work focuses on ways to increase the sample quality in terms of the amount of noise in the image and the intensity difference between the fore- and the background.
The foreground of a shoeprint image is considered as shoeprint region, those areas where the impressions of the shoe outsole pattern are visible.
The goal is to enhance the shoeprint in the image which consists of parts of or the entire shoe outsole pattern of the original shoe.   
\par
Since the state-of-the-art methods, presented further in the following chapter, are evaluated on different datasets, a comparison between them is a challenging task.
Furthermore, as seen in Table \ref{tab:ref:alg}, the discussed publications focus on the definition of a robust descriptor and a powerful matching algorithm to overcome the problem of versatile appearance of the shoeprints.
In this thesis three prototypical preprocessing techniques are developed and tested to enhance the shoeprint images and to make the extracted features accurate. 
\par
For evaluation and testing the FID-300 database is used, because the dataset contains both reference prints as well as real shoeprints secured by the police at crime scenes.
Additionally, the Ground Truth connecting each crime scene sample to a matching reference print is available.
The goal of this work is to define an image processing pipeline which correctly enhances the shoeprint impression and eliminates or suppresses the noise in the shoeprint images.
A secondary objective is to gain an overview of the performance of the algorithms, and make an estimation which methods are applicable in real-life scenarios based on their performance on the FID-300 database. 

%\afterpage{
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00182.jpg}
    \subcaption{}
    \label{fig:intro:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00182_filtered.jpg}
    \subcaption{}
    \label{fig:intro:enhanced}
  \end{subfigure}
  \caption{Result of the proposed algorithm. The background is correctly masked and the contrast between fore- and background is increased. On the bottom right area a weakness of the algorithms is shown, where small pattern structures were wrongly eliminated. \subref{fig:intro:orig} Example image from the FID-300 database; \subref{fig:intro:enhanced} Enhanced image}
  \label{fig:example}
\end{figure}
%}

\section{Challenges}
\par
An obstacle in the topic of shoeprint enhancement and in automatic shoeprint matching in general is the versatile appearance of the shoeprint \cite{kong2017cross}.
%There are approaches available which build models for given structures of the shoeprint \cite{tang2010footwear}, \cite{alizadeh2017automatic}, but they are limited to given parts of the shoe sole pattern and are tested on a restricted database.
Moreover, there is versatile noise of sources listed in the following.
Firstly, the produced shoeprint and its background depend on the properties of the ground where the impression was made, such as the roughness and unevenness of a given type of surface \cite{shor2018inherent}.
%Additionally, the possible unevenness of the ground appears as noise on the final sample.
The shoeprint impressions of Figure \ref{fig:int:varying:3} and \ref{fig:int:varying:20} were made on an even surface whereas on Figure \ref{fig:int:varying:17} and \ref{fig:int:varying:25} cluttered background is visible caused by the surface properties.
Furthermore, objects on the surface, above or behind the shoeprint potentially cover or distort the original pattern, or prevent a clear impression of the complete area of the original shoe sole.
On Figures \ref{fig:int:varying:9} and \ref{fig:int:varying:66} only partial shoeprints were secured, on the left side of Figure \ref{fig:int:varying:66} the structures of the shoe sole pattern are hardly visible because of the additional noise.
Due to the noise, it is possibly ambiguous, which parts of the image are part of the shoeprint, which region is a depiction of the original outsole pattern, thus the fore- and the background of the shoeprint image is not necessarily clearly distinguishable.
Since the original shoeprint image without noise is not available, it is possible that with  suppressing noise the shoeprint is also damaged.
Other than that, illumination changes occur as well \ref{fig:int:varying:20}.
%Besides that, the pattern on the original shoe can also be altered over time due to wear.
%These alterations possibly contain valuable information about the owner, however, they make it more difficult to match the shoeprint image with its reference.
Additionally, there are three common shoeprint securing methods, listed by Katireddy et al. \cite{katireddy2017novel}, producing different results for the same shoeprint impression, these are adhesive lifter, gelatine lifter and electrostatic dust-print lifting device. 
The shoeprint securing technique used depends on the properties of the ground  \cite{katireddy2017novel}. 
The securing method and the additional properties of the floor, for example if it was clean or dusty when making an impression, determine if the positive \ref{fig:int:cap:pos} or the negative \ref{fig:int:cap:neg} image, the shoe outsole pattern or the space between its edges, is captured.
Furthermore, the publications discussed in the following chapter focus on the development of robust feature sets and matching algorithms and not on the preprocessing of the shoeprint samples, as seen in Table \ref{tab:ref:alg} five out of six algorithms introduced for FID-300 do not suggest preprocessing.
Therefore there  is limited research available of the specific domain of shoeprint enhnacement for real life shoeprint images of the FID-300 dataset.
\par
As mentioned previously the state-of-the-art methods were tested and evaluated on different datasets, thus a comparison between them is difficult \cite{rida2019forensic}.
Moreover, the datasets used are not necessarily public \cite{katireddy2017novel}, \cite{dardi2009texture} making it impossible to reproduce the result in such cases.
Additionally, the handcrafted databases allow such restrictions and modifications that do not correlate with real-life scenarios \cite{rida2019forensic}, for instance \cite{dardi2009texture}, \cite{tang2010footwear}.
The samples used can either be synthetically generated, they are secured in a laboratory for the purpose of evaluating forensic image processing algorithms, and computationally distorted \cite{de2005automated}, \cite{gueham2008automatic} or exclude images by a given criteria, for example quality and noise \cite{dardi2009texture}, \cite{tang2010footwear}.
The time consuming approach of re-implementation of the available algorithms and testing them on the FID-300 is also impossible in these cases, since the original dataset of the publication is not accessible and thus any reimplementation can not be verified.
Thus it is challenging to plan a new algorithm based on the published results because the lack of a uniform baseline.

\section{Contribution}
\par
In this thesis an overview of shoeprint enhancement methods is given.
Three approaches are implemented, discussed and evaluated.
Two ways to increase the quality, defined as the contrast between the fore- and the background, of a given shoeprint image are to enhance the pattern regardless of the noise and to suppress the noise.
Since no annotation of the shoeprint images of FID-300 is available, due to the noise, the definition of foreground, as the shoeprint in the shoeprint image, is however possibly ambiguous.
Along fully-automated methods semi-automated algorithms are also considered.
Three different approaches are introduced and examined in respect to their performance on real-life image samples.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.38\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00219.jpg}
	\caption{}
	\label{fig:int:cap:pos}
  \end{subfigure}
  \begin{subfigure}[t]{0.38\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00217.jpg}
	\caption{}
	\label{fig:int:cap:neg}
  \end{subfigure}
  \caption{Example shoeprint impressions from FID-300 where the positive \subref{fig:int:cap:pos} and the negative \subref{fig:int:cap:neg} image of the same shoeprint is captured}
  \label{fig:int:cap}
\end{figure}

\par
Finally, a semi-automated framework is presented which is evaluated on the FID-300 database.
In the first step user input specifying the noise is required.
The input is separated into tiles, and the subparts are compared based on the Fourier-Mellin features of the given region and of the neighborhood selected by the user.
In that way, the background is separated from the foreground and a noise model based on the background is determined.
Since noise appears on the separated foreground as well, those parts are corrected according to the calculated noise model.
After that, the pixels are classified based on their gradients and the non-local means algorithm is applied.
According to the cardinality of each class shoeprint and noise classes are determined.
Finally, candidates of the latter are eliminated.
The final image is thresholded to create a binary image, where the shoeprint is better recognizable than in the original shoeprint image because the noise is suppressed on the foreground, where the shoeprint is assumed to be located, and eliminated on the detected background area.
Throughout the whole processing pipeline morphological operations and small structure elimination are applied multiple times. 
First when a mask for background is built, and also in the end of the pipeline to eliminate small inconsistencies on the determined foreground and detected lines. 
The noise elimination is based on two assumptions.
First, the area selected by the user is representative for the entire image and there is a significant contrast between background and shoeprint areas.
Second, the edges of the shoeprint share similar properties, such as gradient information, frequency as well as overall area and length, and these features are different from those deriving from cluster of the image.
Figure \ref{fig:example} shows an example from the FID-300 database, see \ref{fig:intro:orig}, and the enhanced image, see \ref{fig:intro:enhanced}, using the above algorithm.
\par
The background of the shoeprint image is masked successfully and the contrast between the shoeprint and the background is increased.
However, in the bottom right corner of the image the weakness of the enhancement method is also shown.
Small line structures of the shoe sole pattern resemble the short and fine edges of the noise in the image and are wrongly eliminated.
The detailed evaluation of the proposed algorithm is presented in Chapter 4.
Experimental results show that the enhanced images are clearer, the background is successfully separated and the shoeprint is less noisy than on the original images.
Moreover, the improved images have a better matching rate than their original version according to the experiments conducted on the enhanced images, on the original samples and on the reference images using three common image features such as Fourier-Mellin, SIFT and SURF.
\par
In this thesis three algorithms are presented for enhancement of shoeprint images.
Two of them are fully-automated, whereas in the case of the third one user input is required.
Based the evaluation of their performance on shoeprint images from the FID-300 dataset, an overview of the applicability of the enhancing methods for real-life shoeprint images is given.
Their advantages and limitations are examined.
Finally, possible ways for improvement are discussed as well.

\section{Structure of the Work}
\par
To gain an overview of the published research the following section, Chapter 2, gives a review of the literature. 
Along papers published on the topics of shoeprint identification, matching and enhancement, research on similar domains is presented as well.
Consequently, the field of fingerprint processing is also overviewed for possibilities of utilizing their solutions in the given problem space.
Furthermore, natural image enhancement and denoising techniques are revised as well.
\par
In Chapter 3 the approaches for enhancement are described.
The first section presents and reviews an algorithm for detecting shoeprints on shoeprint images under varying conditions.
The second one describes an automated noise suppression pipeline.
In the last section an algorithm for enhancing real-life crime scene shoeprint images from FID-300 is proposed.
Details on the implementation are given as well.
\par
In Chapter 4 experimental results are shown and the proposed algorithms are evaluated whether they are applicable for real-life forensic images.
In Chapters 5 prospective future work is discussed and the final conclusion is given. 

\chapter{Related Work}
\par
In order to find and develop an effective algorithm for shoeprint image enhancement, an overview about relevant research is made first.
Along the literature of image enhancement and noise removal, image descriptors are also reviewed.
Discriminative image descriptors are considered as well to gain better insight and to define an approach which is optimized for the rest of the shoeprint identification pipeline.
In this chapter the research in the domain of shoeprint identification is reviewed.
Other than that, publications from similar domains such as fingerprint and palmprint detection are discussed as well.
Research for fingerprint identification has been chosen for review because of their similar goal of edge pattern recognition.
Moreover, an overview of techniques from the field of natural image enhancement and description along with general image denoising is also given.
This chapter is separated into three parts, first, image enhancement techniques are described, after that algorithms developed for noise removal specifically are discussed, and lastly, proposed image descriptors are reviewed.


\section{Image Enhancement}
\label{sec:rw:ImageENhancement}

In this section image enhancement techniques from three specific domains are discussed, these are shoe- and fingerprint identification and natural image enhancement.

\subsection{Shoeprint Enhancement}

\par
The problem definition and the use-case of the different publications in the topic of shoeprint identification vary strongly \cite{rida2019forensic}.
Because of the absence of a common database, the algorithms discussed are separated into two groups, techniques tested on synthetic samples, i.e. on shoeprint impressions generated for the purpose of evaluation of shoeprint identification algorithms, and on real-life shoeprint images.
Furthermore, a group of algorithms developed for real crime-scene shoeprint images makes restrictions about the input image and exclude images by a predefined criteria, for example noise and quality.
Figure \ref{fig:rw:database} shows example images from a synthetic \ref{fig:rw:synthetic}, from a restricted \ref{fig:rw:restricted} and high \ref{fig:rw:highFID} and low quality samples \ref{fig:rw:lowFID} from the FID-300 dataset.
Tables \ref{tab:ref} and \ref{tab:ref:alg} provide an overview of the datasets used of the algorithms discussed.
The former contains further information about the accuracy of the algorithms, the latter gives keywords about the proposed methods of the publications.
The algorithms presented in the "real" section of the tables were tested on the FID-300 dataset, used in this thesis as well.
Table \ref{tab:ref} also gives information about the matching accuracy of the algorithms discussed.
The accuracy is given in the format of X\%@Y(\%), where X is the probability of the corresponding reference image being among the top Y matches. 
10\% means the probability of having the corresponding reference image among the top 10\% of all matched images, 10 stands for being the corresponding image among the top 10 best matches.
If no Y is given, the accuracy stands for the probability of providing the corresponding reference image as first match.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/synthetic.jpg}
    \subcaption{}
    \label{fig:rw:synthetic}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/restricted.jpg}
    \subcaption{}
    \label{fig:rw:restricted}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/00025.jpg}
    \subcaption{}
    \label{fig:rw:highFID}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/00174.jpg}
    \subcaption{}
    \label{fig:rw:lowFID}
  \end{subfigure}
  \caption{Example images of the different datasets of the forensic shoeprint matching algorithms
	\subref{fig:rw:synthetic} Example image from a synthetic dataset \cite{alizadeh2017automatic}; \subref{fig:rw:restricted} Example image from a real crime scene dataset excluding low quality images \cite{li2014retrieval}; \subref{fig:rw:highFID} Example high quality image from the FID-300 database \cite{kortylewski2014unsupervised}; \subref{fig:rw:lowFID} Example low quality image from the FID-300 database \cite{kortylewski2014unsupervised}}
  \label{fig:rw:database}
\end{figure}


\par
Morphological Operations, namely Opening and Closing, are popular techniques for improving the quality of both kind, realistic and synthetic, of shoeprint images, they are  used in many cases \cite{wang2014automatic}, \cite{kong2014novel}, \cite{li2014retrieval}.
Wang et al. \cite{wang2014automatic}, Kong et al.  \cite{kong2014novel}, Li et al. \cite{li2014retrieval} and  Wu et al. \cite{wu2019crime}use the Morphological Operations to correct inconsistencies after thresholding.
In case of the algorithm proposed by Tang et al. \cite{tang2010footwear}, the input is binarized with Canny edge detection, the inconsistencies are eliminated afterwards again with Opening and Closing.
However, as seen on Table \ref{tab:ref:alg}, Morphological Operations are mainly used on high quality data.
They are applied on binarized data, but, except of Wu et al. \cite{wu2019crime}, the other discussed algorithms tested on FID-300 use no enhancement methods and focus on the definition of robust descriptor instead.
Furthermore, Morphological Operations are only used for small corrections in the presented publications, see Figure \ref{fig:rw:restricted}, but on FID-300 samples noise causes bigger distortions than Opening and Closing are able to handle, see Figure \ref{fig:rw:lowFID}.

\begin{minipage}{\linewidth}
\begin{tabular}{c|c|c|c}
Algorithm & Kind of Dataset & Size of Dataset & Accuracy \\
\specialrule{2.5pt}{1pt}{1pt}
Algarni et al. (2008) \cite{algarni2008novel} & \multirow{8}{*}{ synthetic } & 500 & 99\% * \\
Gueham et al. (2007) \cite{gueham2007automatic} & & 100 & 100\% * \\
Gueham et al. (2008) \cite{gueham2008automatic} & & 500 & 99\% @ 10 * \\
Alizadeh et al. (2017) \cite{alizadeh2017automatic} & & 190 & 99\% * \\
Wang et al. (2014) \cite{wang2014automatic} & & 210000 & 91\% @ 2\% * \\
Nibouche et al. (2009) \cite{nibouche2009rotation} & & 300 & 90\% * \\
Zhang et al. (2005) \cite{zhang2005automatic} & & 512 & 98\% @ 4\% * \\
Patil et al. (2009) \cite{patil2009rotation} & & 1400 & 91\% * \\
\hline
Almaadeed et al. (2015) \cite{almaadeed2015partial} & \multirow{6}{*}{ restricted } & 300 & 99\% * \\
Kong et al. (2014) \cite{kong2014novel} & & 104 & 64\% @ 20 \\
Li et al. (2014) \cite{li2014retrieval} & & 195 & 94\% \\
Tang et al. (2010) \cite{tang2010footwear} & & 2660 & 71\% @ 1\% * \\
Dardi et al. (2009) \cite{dardi2009texture} & & 87 & 73\% @ 10 * \\
Katireddy et al. (2017) \cite{katireddy2017novel} & & 800 & no matching done \\
\hline
Wu et al. (2019) \cite{wu2019crime} & \multirow{6}{*}{ FID-300 } & 300/1175 & 93\% @ 2\% \\
Wu et al. (2019) \cite{wu2019losgsr} & & 300/1175 & 68\% @ 1\% \\
Kong et al. (2017) \cite{kong2017cross} & & 300/1175 & 90\% \\
Kong et al. (2019) \cite{kong2019cross} & & 300/1175 & 86\% @ 1\% \\
Kortylewski et al. (2014) \cite{kortylewski2014unsupervised} & & 300/1175 & 86\% @ 20\% * \\
Kortylewski et al. (2016) \cite{kortylewski2016probabilistic} & & 300/1175 & 71\% @ 20\% * \\
\end{tabular}
\captionof{table}{Overview about the database and the performance of the algorithms proposed for forensic image matching. The entries labeled with "*" originate from \cite{rida2019forensic}}
 \label{tab:ref} 
\end{minipage}


\begin{sidewaystable}
\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|c|c|c}
Algorithm & Kind of Dataset & Enhancement Method & Descriptor \\
\specialrule{2.5pt}{1pt}{1pt}
Algarni et al. (2008) \cite{algarni2008novel} & \multirow{8}{*}{ synthetic } & Otsu th. & Hu moments \\
Gueham et al. (2007) \cite{gueham2007automatic} & & Bandpass filtering & Fourier Transf. with Phase Only Corr. \\
Gueham et al. (2008) \cite{gueham2008automatic} & & - & Fourier-Mellin Transf. \\
Alizadeh et al. (2017) \cite{alizadeh2017automatic} & & Otsu th., Median filter & Sparse representation \\
Wang et al. (2014) \cite{wang2014automatic} & & Morphological op., Adaptive th. & Fourier-Mellin Transf. \\
Nibouche et al. (2009) \cite{nibouche2009rotation} & & - & Harris and SIFT \\
Zhang et al. (2005) \cite{zhang2005automatic} & & Partial Differential Equations & Fourier Transf. \\
Patil et al. (2009) \cite{patil2009rotation} & & - & Gabor Transform \\
\hline
Almaadeed et al. (2015) \cite{almaadeed2015partial} & \multirow{6}{*}{ restricted } & - & Hessian Detector \\
Kong et al. (2014) \cite{kong2014novel} & & Morphological op., Otsu th. & Gabor Transff, Zernike Features \\
Li et al. (2014) \cite{li2014retrieval} & & Morphological op., Adaptive th., Bandpass filtering & Gabor Transf. \\
Tang et al. (2010) \cite{tang2010footwear} & & Morphological op., Canny edge detection & Hough Line Transf. \\
Dardi et al. (2009) \cite{dardi2009texture} & & - & Power Spectral Density \\
Katireddy et al. (2017) \cite{katireddy2017novel} & & SMQT, Daubechies wavelets & - \\
\hline
Wu et al. (2019) \cite{wu2019crime} & \multirow{6}{*}{FID-300 } & Morphological op. , Otsu th. & Gabor Transf., Fourier-Mellin Transf. \\
Wu et al. (2019) \cite{wu2019losgsr} & & - & Manifold ranking \\
Kong et al. (2017) \cite{kong2017cross} & & - & CNN \\
Kong et al. (2019) \cite{kong2019cross} & & - & CNN \\
Kortylewski et al. (2014) \cite{kortylewski2014unsupervised} & & - & Fourier Transf \\
Kortylewski et al. (2016) \cite{kortylewski2016probabilistic} & & - & Active Basis Model \\
\end{tabular}
\captionof{table}{Overview about the database and the proposed methods for enhancement and feature descriptors of algorithms proposed for forensic image matching.}
\label{tab:ref:alg} 
\end{minipage}
\end{sidewaystable}

\par
To create a binary image and eliminate noise various thresholding techniques are used.
Otsu  \cite{wu2019crime}, \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{kong2014novel} and adaptive thresholding \cite{wang2014automatic}, \cite{li2014retrieval} are two popular algorithms.
Wang et al. \cite{wang2014automatic} and Wu et al. \cite{wu2019crime} combine Otsu thresholding with a grid based approach to calculate exact thresholds for every subarea of the picture.
Similar to the Morphological Operations, thresholding is preferred on high quality images, see Table \ref{tab:ref:alg}.
Wu et al. \cite{wu2019crime} adjust the algorithm for lower quality images by calculating several thresholds for subregions of the image.
Nevertheless, the neglect of thresholding methods for real datasets indicates that they are not applicable for the changing conditions on the samples of the FID-300 dataset.
     
\par
Another way to eliminate noise is image filtering.
Alizadeh et al. \cite{alizadeh2017automatic} use a simple Median filter.
Zhang et al. \cite{zhang2005automatic} take advantage of the Partial Differential Equations approach.
In this way the edges are preserved while the background is smoothed according to a controlled curvature motion criteria. 
Katireddy et al. \cite{katireddy2017novel} use Successive Mean Quantization Transform (SMQT) \cite{nilsson2013smqt} as one step to enhance the real-life database.
Figure \ref{fig:rw:SMQT} shows the output of the SMQT algorithm on an example image.
There are, thus two proposed ways for filtering, the first one is general smoothing, the second one is edge preserving filtering.
Comparing the databases the proposed algorithms were tested on, the latter is more suitable for forensic images of FID-300 for two reasons.
Smoothing is sufficient for images such as Figure \ref{fig:rw:synthetic} where the sample is clear and the shoeprint pattern consists of strong edges \cite{alizadeh2017automatic}, but it potentially damages the pattern information on lower quality samples such as in Figure \ref{fig:rw:lowFID}.
Furthermore, Katireddy et al. \cite{katireddy2017novel} tested their algorithm on restricted real samples, which is more similar to FID-300 than synthetic images. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/SMQTorig.jpg}
    \subcaption{}
    \label{fig:rw:SMQTin}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/SMQT.jpg}
    \subcaption{}
    \label{fig:rw:SMQTout}
  \end{subfigure}
  \caption{Example image presenting the enhancement feature of the SMQT algorithm \cite{katireddy2017novel};
		\subref{fig:rw:SMQTin} Example shoeprint impression; \subref{fig:rw:SMQTout} Enhanced image with the SMQT algorithm}
  \label{fig:rw:SMQT} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Bandpass operators are also used for noise suppression.
The images are converted to the frequency domain where high and low frequencies are eliminated, as proposed by Gueham et al. \cite{gueham2007automatic},  Richetelli et al. \cite{richetelli2017classification}.
In the algorithm of Li et al. \cite{li2014retrieval} only the lower frequencies are eliminated.
Another frequency based approach was proposed by Katireddy et al. \cite{katireddy2017novel} for real dataset based on Daubechies wavelets.
After SMQT enhancement the Daubechies wavelets are used to separate the fore- and background and to remove the noise in the latter.
In case of FID-300, it is challenging to define a heuristic for noise as well as for the shoeprint because of its versatile appearance and no previous knowledge about their properties.
Therefore, Bandpass filtering potentially damages the pattern information.
However, separating the fore- and the background where the background does not contain shoeprint areas, as Katireddy et al. \cite{katireddy2017novel} propose, makes it possible to process the noise and the shoeprint independently.
Furthermore, it also gives a sample about the appearance of noise on a given image, thus a calculation of a noise model is possible.

\par
It is outstanding that the majority of reviewed publications working on the FID-300 dataset do not suggest to use enhancement techniques, as shown in Table \ref{tab:ref:alg}.
As illustrated on Figure \ref{fig:int:varying}, the appearance of noise changes greatly, thus, it is challenging to define a universal algorithm which deals with the  varying noise accurately.
Additionally, the presented algorithms in Table \ref{tab:ref}, do not use a-priori knowledge about the samples, as no information about the appearance of noise nor of the shoeprint is available.
Because of that, they focus on the definition of robust descriptor and powerful matching technique which finds the corresponding shoeprints, despite the low shoeprint shoeprint-to-noise ratio and changing conditions.

\subsection{Fingerprint Enhancement}
\par
Bandpass \cite{zhou2011adaptive}, \cite{baig2015enhancement} and general image filtering  \cite{jahan2017robust} are used in the field of fingerprint enhancement.
Zhou et al. \cite{zhou2011adaptive} uses a low- and a highpass filter to eliminate striking frequencies. 
Baig et al. \cite{baig2015enhancement} apply Directional Hilbert transform of Butterworth bandpass to collect the different phase shifts and eliminate the artifacts created by previously thresholding the input.
Wang et al. \cite{wang2014enhanced} decompose the image into four subbands and process them separately, calculating the noise for every subband respectively.
Jahan et al. \cite{jahan2017robust} apply Fuzzy filtering followed by thinning.
Fuzzy filtering is a local method to preserve the edge information and fine line structures while suppressing the noisy background of the input.
Similar to forensic shoeprint samples, fingerprint images also depict a 2 dimensional object where the information derives from the line pattern of the sample, see Figure \ref{fig:rw:fingerprint}.
The use of bandpass filtering and signal transform for fingerprint images shows, that the edge content of the input does not influence their performance, assuming that the noise and the content of the given image have different frequencies.
The proposed methods indicate that they are suitable also for fine-lined images, such as finger- or palmprint samples \ref{fig:rw:fp}, which is promising when using them on shoeprint images with detailed and fine-lined pattern as well \ref{fig:rw:ref}. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/fingerprint.jpg}
    \subcaption{}
    \label{fig:rw:fp}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/00241.jpg}
    \subcaption{}
    \label{fig:rw:ref}
  \end{subfigure}
  \caption{Similarity between a fingerprint sample and a shoeprint  image;
		\subref{fig:rw:fp} Fingerprint image \cite{van2016fingerprint}; \subref{fig:rw:ref} Fine-lined shoeprint pattern from FID-300  \cite{kortylewski2014unsupervised}}
  \label{fig:rw:fingerprint} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

%\section*{Tattoo Enhancement}

%For tattoo enhancement an algorithm from Han et al. \cite{han2013tattoo} was proposed which combines Gaussian filtering with Hysteresis thresholding. 
%Hysteresis thresholding is a neighborhood-aware approach where a pixel is labelled when it is above a given low threshold and simultaneously connected to other pixels meeting a higher thresholding criteria.
%Acton et al. \cite{acton2008matching} propose to use Active Contour Model to find the boundaries of tattoo images and apply Opening and Closing as well to eliminate small inconsistencies.

\subsection{Natural Image Enhnacement}
\par
Maini et al. \cite{maini2010comprehensive} published a review about natural image enhancing algorithms and defined two main groups of them, Frequency and Spatial Domain Methods.
First, publications utilizing techniques from the former group are discussed, followed by a review of Spatial Domain Methods. 
\par
Xu et al. \cite{xu2016image} combine Bandpass filtering with adaptive thresholding.
Similar to Wang et al. \cite{wang2014enhanced} the image is separated into four subbands, and the threshold is calculated for every image separately.
Sugamya et al. \cite{sugamya2016image} apply Subband Decomposition with two staged Histogram Equalization.
The histogram of the input is equalized globally first, after that it is decomposed into subbands to equalize the values locally for every of the four generated subimages.
In these methods the subbands are enhanced separately and reconstructed into the final output in the end.
With processing the shoeprint images on different subbands, a method specialized for noise on the given frequency is defined.
Even though the appearance of noise varies across the samples, with decomposition it is separated into known subbands.
This way a filtering method for general noise is built by breaking it down into known parts, into known subbands.  
\par
Median Filters are used not only in the domain of shoeprint enhancement \cite{alizadeh2017automatic}, but also for natural image noise suppression.
Apart from Median Filter, Li et al. \cite{li2014rapid} utilize Average and Wiener Filter to suppress the occurring noise and to prepare the input for neighborhood based feature extraction.
Feng et al. \cite{feng2011bag} propose a Bag-of-Words algorithm based on the Gabor wavelets of the input. 
For preprocessing, the Watershed Transform is used.
With Wiener Filter and Bag-of-Words method a representation of the estimated noiseless image is built, and with filtering this model is approximated.
Along edge preserving filter this approach is favorable, when enhancing shoeprint images with varying noise.
In this way, regardless of the appearance of the clutter and non-content elements, the original image is estimated, thus the non-content, in case of this thesis the non-shoeprint, parts are suppressed.
The performance of these methods, however, depends on the underlying estimation of the original image \cite{li2014rapid}.
\par
Histogram Operations are combined apart from Bandpass filtering, like Sugamya et al. \cite{sugamya2016image} propose, also with Thresholding as suggested by Yao et al. \cite{yao2016image}. 
Their approach first separates the histogram of the input into two parts using the Otsu's method, then equalizes the histogram of the generated subimages. 
Figure \ref{fig:rw:BHNMT} shows the results of the above algorithm on an example image.
As Figure \ref{fig:rw:BHNMT} illustrates, this method is more suitable for dehazing tasks. 
Even though, there are low-contrast images in FID-300 available, such as Figure \ref{fig:int:cap:neg}, the artifact is not derived from haze.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/BHNMTorig.jpg}
    \subcaption{}
    \label{fig:rw:BHNMTin}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/BHNMT.jpg}
    \subcaption{}
    \label{fig:rw:BHNMTout}
  \end{subfigure}
  \caption{Example image of the enhancement feature of the algorithm proposed by Yao et al. \cite{yao2016image};
			\subref{fig:rw:BHNMTin} Example low-contrast image; \subref{fig:rw:BHNMTout} Enhanced image with the histogram thresholding algorithm}
  \label{fig:rw:BHNMT} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Color processing techniques are also used for natural image enhancement: it is applied for image dehazing at low contrast images, \cite{singh2018dehazing} and also for regular noise removal \cite{ren2018joint}, \cite{zhang2016simultaneous}. 
Bhairannawar et al. \cite{bhairannawar2017color} switch from RGB to HSV and use Laplace filter to detect regions with intensity changes. 
During the processing, the H channel is not modified to prevent color distortion artifacts.
Although color processing is a well researched field with promising solutions, no wider overview of this topic is given in this thesis, since the FID-300 dataset provides only grayscale images. 
An example of a shoeprint impression dataset with colored samples can be found at \cite{katireddy2017novel}. 

\section{Noise Removal}

Noise Removal methods reviewed in this section are based on estimating the original image and eliminating the deviating features of the data.
One way is denoising through gradient histogram preservation \cite{zuo2013texture}.
The distribution of gradients  of the original image is estimated first, and the noisy image is adjusted to the calculated values.
An alternative way is to decompose a single image and based on the clear parts, approximate the noisy regions.
Huang et al. \cite{huang2013self} propose a self-learning algorithm that only considers the high frequency parts of the decomposed image.
In \cite{xu2015patch}, \cite{talebi2013global}, \cite{chatterjee2011patch} and \cite{guo2015efficient} images  are separated spatially instead of in the frequency domain, the techniques are based on the idea of non-local means, where the pixels are clustered according to a given criteria and they are set to the mean of the members belonging to the same cluster.
The difference between the previous algorithms is how they cluster the pixels or regions into different classes.
Taleby et al. \cite{talebi2013global} uses an iterative shrinkage strategy. 
Chatterjee et al.  \cite{chatterjee2011patch} group the geometrically similar regions and estimate the noise for every class separately with the Wiener filter, whereas Guo et al. \cite{guo2015efficient} utilize Block Matching to determine cluster memberships. 
Additionally, the spatial location is considered as well while calculating the mean value in a given class.
The members are weighted according to their distance to the current region.
Offering adjustable classification criteria, the non-local means method is promising for the enhancement of forensic shoeprint impressions.
If a feature is defined which is different between noise and shoeprint elements but similar within those, with non-local means the two components are processed separately.
In the next section proposed feature descriptors are reviewed, such as an overview of possible classification criteria can be made.

\section{Image Descriptors}

This part is subdivided into three domains offering solutions for image description in different areas.
Similar topics are reviewed to gain insight about the powerful descriptors and to consider whether they can be used in the domain of shoeprint enhancement.
Shoeprint descriptors are described first, followed by the fingerprint features.
Finally, at the end of the section natural texture descriptors are also reviewed. 

\subsection{Shoeprint Descriptors}
\par
Since the design of image descriptors depends on the data it is invented for the properties of the database the given approach was tested on is given in Tables \ref{tab:ref} and \ref{tab:ref:alg}.
Patil et al. \cite{patil2009rotation} propose to use the Radon Transform to determine the dominant direction of structures on the shoeprint image and to process the aligned input with Gabor Transform.
A feature map is built by convolving the input image with the Gabor filter.
Kong et al.  \cite{kong2014novel} combine the Gabor features of the shoeprint image with Zernike features to describe the shapes in the shoeprint.
Li et al. \cite{li2014retrieval} suggest to use the histogram extracted in the Gabor transformed domain as descriptors.
In the approach published by Wu et al.  \cite{wu2019crime}, Gabor Filters are combined with Haar Wavelets and Fourier-Mellin Transform to get an integrated, multi-level descriptor. 
In other publications Fourier-Mellin Transform is proposed as well, e.g. Gueham et al. \cite{gueham2008automatic} where after transforming the input into the Fourier domain and mapping it into Log-Polar coordinates a correspondence scheme between images is suggested.
Wang et al. \cite{wang2014automatic} apply multiresolution matching based on  Fourier-Mellin Transform.
The correlation coefficient is calculated by comparing the Fourier-Mellin features of multiple resolutions.
Richetelli et al. \cite{richetelli2017classification} classify shoeprint images by applying Fourier-Mellin Transform following the calculation of Phase Only Correlation (POC) to determine the translative difference between two images in the frequency domain.
Gueham et al. \cite{gueham2007automatic} suggest to use the basic Fourier Transform before calculating the POC in another paper.
Kortylewsky et al. \cite{kortylewski2014unsupervised} also propose a Fourier Transformation based method.
It is an unsupervised learning approach, where the periodic structures of a shoeprint are compared in the Fourier domain.
Richetelli et al. \cite{richetelli2017quantitative} compares Randomly Acquired Characteristics of shoeprints, e.g. small damages, modifications and stuck objects on or in the shoesole pattern, examining their Fourier descriptor.
Other than Fourier-like transformations, the use of Power Spectral Density was proposed in \cite{dardi2009texture}.
It is a descriptor for random, broadbrand signals based on the the frequency and power.
In Table \ref{tab:ref} the matching accuracy of the descriptors above is also listed.
Comparing that value, the Fourier-Mellin features outperform the Gabor and the Fourier Transform, having a high accuracy of	 99\% and 100\%.
However, the  evaluation was done on different datasets, the above 95\% accuracy was reached on synthetic images.
Two algorithms \cite{kortylewski2014unsupervised} and \cite{wu2019crime} were tested on FID-300 and the latter achieved 93\% accuracy by the combination of Gabor and Fourier-Mellin transform.
This shows, that Fourier domain based features are powerful descriptors for synthetic images, and they are also applicable for lower quality data, such as the shoeprint images of FID-300.
\par
%In high quality, so synthetic or restricted datasets shape descriptors are a popular choice for feature extraction.
Algarni et al. \cite{algarni2008novel} suggest to use Hu moments because of their robustness against noise, rotation and resolution.
It is also proposed to combine the feature descriptors, as Kong et al. \cite{kong2014novel} incorporate Zernike and Gabor features.
Tang et al. \cite{tang2010footwear} define their own fundamental shapes based on common basic structures on the shoe outsole pattern through Hough Line Transform. 
The extracted features are then stored in an Attributed Relational Graph to represent the entire shoeprint.
Comparing the datasets the algorithms were introduced for, see Table \ref{tab:ref}, it is noticeable that shape descriptors were only introduced for high-quality data and not for samples from FID-300.
In FID-300 images, there is a high amount of noise which distorts the original shoeprint pattern.
In this case, shape descriptors are less suitable, because the original outlines of the elements are modified or covered by noise.
\par
Point descriptors were also proposed for shoeprint impression matching.
Nibouce et al. \cite{nibouche2009rotation} propose to use a four-level Harris Detector and combine it with SIFT.
Almaadeed et al. \cite{almaadeed2015partial} use the same combination and uses the Hessian Detector additionally for the same purpose in case of restricted real-life datasets.
Another publication \cite{richetelli2017classification} extracts the SIFT features from the frequency domain after applying Fourier-Mellin Transformation and POC on the input image.
Similar to the shape descriptors, point descriptors were tested only on high quality data, see Table \ref{tab:ref:alg}.
Even though SIFT is a robust descriptor  \cite{lowe1999object}, its neglect by algorithms introduced for FID-300 indicates, that it does not overcome the issue of low signal-to-noise, where the signal is the shoeprint, ratio on the shoeprint images.
\par
Kong et al. \cite{kong2017cross}, \cite{kong2019cross} define a descriptor for real crime scene shoeprint images extracting the mid-level features from a Convolutional Neural Network.
Similarly, Wu et al .\cite{wu2019losgsr} also use machine learning to calculate opinion scores for given matching pairs of real shoeprint images.
In the learning process manifold ranking is used where the opinions of human experts as well as feature similarities are both incorporated.
Kortylewsky et al. \cite{kortylewski2016probabilistic} developed a hierarchical composition of Active Basis Models, and also extended this for natural image environments \cite{kortylewski2019greedy}.
The algorithms presented in this section were all tested on FID-300.
Comparing to other descriptors previously discussed in this section, it is notable that the majority of publications utilize machine learning.
That implies that for a robust descriptor training is indispensable, and to deal with the varying and high amount of noise the descriptor needs to be adjustable and adapted to the given shoeprint images.

\subsection{Fingerprint Descriptors}
\par
Signal domain representations are attractive not only for shoeprint but also for fingerprint description.
Multi-resolution representation through Gabor expansion was proposed by Bolle et al. \cite{bolle2012fingerprint} to achieve a localized texture descriptor.
Van et al. \cite{van2016fingerprint} use Adaptively Adjusted Modified Finite Radon Transform after Gabor filtering to connect edges and eliminate inconsistencies.
Rida et al. \cite{rida2018palmprint} propose an ensemble descriptor consisting of Gabor filter, Local Binary Pattern, Histogram of Oriented Gradient and Line Detector to represent a palmprint impression.
Other than that, Li et al. \cite{li2012texture} suggest to use Directional Filter Banks, where the image is separated into eight subimages, which then are further decomposed into two frequencies via Wavelet Transform.
In case of shoeprint images, frequency domain features, such as Fourier-Mellin features were dominating, they were proposed in several publications  \cite{gueham2007automatic}, \cite{wu2019crime} and achieved a high accuracy of 100\% and 93\% respectively, see Table \ref{tab:ref}.
But for fingerprint descriptors Gabor Transform and Radon Transform are preferred.
This indicates that the latter is more suitable for fine-lined, detailed patterns, illustrated on Figure \ref{fig:rw:fp}, and the former is more applicable on bigger geometrical structures, Figure \ref{fig:int:varying}.
This observation also foresights the performance of Fourier-Mellin features on detailed shoe outsole patterns such as shown on Figure \ref{fig:rw:ref}. 
\par
For fingerprint images several point feature descriptors were proposed.
In \cite{zhou2011adaptive} SIFT \cite{lowe1999object} features were fused with Hough Transform and Minutiae Information of the fingerprint.
Chen et al. \cite{chen2013hierarchical} also use the Hough Transform and extend it with a hierarchical voting score to improve the matching information. 
Along  \cite{rida2018palmprint} Ghandehari et al. \cite{ghandehari2012palmprint} recommend to use HOG in a 3-level Gaussian pyramid to estimate the local strength of different types of edges on the image.
Jahan et al. \cite{jahan2017robust} suggest to combine the Minutiae Information with Speeded Up Robust Features (SURF) and compare them with a Neural Network to find the matching pairs.
Local descriptors are suitable for fingerprint images because of the compactness of information.
That is, however, not guaranteed on forensic shoeprint samples.
Depending on the original shoesole pattern, the elements of the shoeprint are possibly bigger and widespread on the input image, see Figure  \ref{fig:int:varying}, compared to  fingerprint sample, see Figure \ref{fig:rw:fp}.
Unlike on fingerprints, where mainly minutiae information is considered for identification, the more complex geometrical structures on shoe outsole pattern are relevant when matching shoeprint images.
%\par
%According to the following publications, Local Binary Patterns (LBP) are also suitable for fingerprint description.
%As mentioned previously, Rida et al. \cite{rida2018palmprint} published a combined feature vector that LBP is also a part of.

%\par
%Sparse representations are also used for fingerprint description.
%Rida et al. \cite{rida2018palmprint} stores the hybrid features in that way, whereas Shao et al. \cite{shao2013fingerprint} represents predefined fingerprint patches, called dictionary atoms, in a sparse way.

%\section*{Tattoo Descriptors}
%\par
%For tattoo description, three main methods are overviewed, which are signal domain features, point descriptors, especially SIFT, and shape features, and  Kim et al. \cite{kim2015robust} fuses all three of them. 
%For local descriptor, the Shape Context Features are used, whereas for global descriptor multi-level SIFT and the Fourier Transform is utilized.
%Acton et al. \cite{acton2008matching} built an Active Contour Model which consists of Haar Wavelet, an HSV histogram and a Fourier Shape Descriptor.
%Wavelet tarnsforms and shape features were also introduced for shoprint identification \cite{wang2014automatic}, \cite{algarni2008novel}.
%Using them on tattoo images shows that they are robust against illumination changes and distortions, since tattoos are ommonly . 
%\par
%Other than  \cite{kim2015robust}, there are multiple publications available, such as \cite{duangphasuk2013tattoo}, that take advantage of SIFT features for tattoo image description.
%It is common to combine them with other shape descriptor to have a more detailed representation.
%Han et al. \cite{han2013tattoo} fuse Active Shape Models with SIFT, in \cite{yi2015impact} SURF is added and in \cite{kim2016tattoo} SIFT is extended with the Local Self Similarity measure.
%Duangphasuk et al. \cite{duangphasuk2013tattoo} base their approach mainly on shape description and similar to  \cite{kim2015robust} use Shape Context Features for tattoo representation. %am�gy ezzel az angoloknak az lenne a baja, hogy cask felsorolsz egy sor m�dszert, de igaz�b�l semmit nem tudunk meg arr�l, hogy ezek milyenek. M�rmint, nem azt, hogy mit jelentenek ezek a szavak, hanem azt, hogy mi�rt fontos az, hogy ezeket felsoroljad. Ezek jobb� teszik az munk�t? Ezek nem m�k�dtek? Ezek alapj�n �rod a ti�det? Mi�rt �rdemes egy�ltal�n ezt felsorolni? Mi�rt fontos, hogy ismerjem ezeket a neveket?

\subsection{Natural Texture Descriptors}
\par
For natural texture description signal and frequency domain features are reviewed.
Mistry et al. \cite{mistry2017content}  mix multiple features, mainly color and texture descriptors.
For texture description, Gabor Wavelet and Binarized Statistical Images  \cite{kannala2012bsif} are used simultaneously.
Hatipoglu et al. \cite{hatipoglu2000image} suggest taking advantage of Dual Tree Complex Wavelet transform instead of Gabor Wavelet.
Quevedo et al. \cite{quevedo2002description} and Xu et al. \cite{xu2009viewpoint} suggest Fractal features, because they are invariant to intensity and scale changes.
Xu et al. \cite{xu2009viewpoint} propose using a multifractal spectrum to achieve robustness against viewpoint changes and non-rigid deformations.
Hayati et al. \cite{hayati2018wirif} and Ahonen et al. \cite{ahonen2009rotation} incorporate LBP-like features with the frequency domain representation.
Ahonen et al. \cite{ahonen2009rotation} calculate the LBP of the Fourier features of the image, whereas Hayati et al. \cite{hayati2018wirif} use Wave Inference, where the information of multiple different-sized asymmetric neighborhoods is added respectively.
Fourier Transformation and Gabor wavelets were introduced for all reviewed topics so far \cite{kortylewski2014unsupervised}, \cite{wu2019crime}, \cite{bolle2012fingerprint}.
It implies high versatility and robustness of the descriptors, since it is possible to configure them for shoeprint, fingerprint as well as for natural texture description.  
These descriptors are thus powerful tools, and based on the reviewed literature it is advisable to take advantage of them.
\par
There are several publications available which use LBPs for texture description, e.g. \cite{guo2012discriminative}, \cite{hong2014combining}, \cite{ahonen2009rotation}, or for texture classification, e.g. \cite{khellah2011texture}, \cite{guo2010rotation}, \cite{zhang2017learning}.
Wang et al. \cite{wang2013pixel} modify the usual LBP pipeline with Pixel to Patch sampling to increase the quality of the descriptor without slowing down the calculation.
At Pixel to Patch sampling, the weighted average of the neighboring pixels in a given radius is calculated instead of interpolation.
Additionally, the Local Neighboring Intensity Relationships subtracted from their grey-scale information are also considered.
%LBP is frequently used in combination with other techniques as well, examples for that are discussed in the followings. 
In \cite{hong2014combining} based on the covariance matrix additional discriminative features are calculated. 
In \cite{guo2010completed} LBPs are calculated twice, after that the input is separated into two components, into phase and magnitude, to make the basic LBP rotation invariant as well.
Zhang et al.  \cite{zhang2017learning} propose a learning strategy for adaptively weighting the sign and magnitude LBPs to estimate their contribution in the given area. 
Along the sign and magnitude LBPs, a third modified LBP is also defined, where the local difference vector is determined.
In this way, robustness against illumination changes is achieved.
In \cite{khellah2011texture}, LBP are calculated on multiple levels.
Another solution for rotation invariance is proposed by Davarzany et al. \cite{davarzani2015scale}. 
In their approach a circular neighboring radius and a dominant orientation is stored so that rotation invariance is also achieved.
Li et al. \cite{li2014rapid} process the neighborhood with Rapid Transform, which is robust against cyclic permutation, to achieve the same robustness.
Wang et al. \cite{wang2017local} suggest solving rotation invariance by storing the radial and tangential information instead of the intensity values.
Guo et al. \cite{guo2010rotation} defines "LBPV" instead of LBP, where V stands for variance. 
In their approach the LBP features with high variance are chosen as discriminative, because they indicate high frequency in the related region.
In \cite{bala2016local}, it is proposed to calculate Local Texton Patterns, where the Value channel of the HSV input is subdivided into overlapping subblocks according to its content, and the modified LBP is then determined based on those subregions.
Fadaei et al. \cite{fadaei2017local} published a similar approach called Local Derivative Radial Patterns.
Instead of binary coding, a multi-level coding is used in multiple directions, and the differences between neighbors are weighted additionally. 
Chahi et al. \cite{chahi2018local} define Local Ternary Patterns which store the directional patterns explicitly.
Even though, LBPs are extensively researched for natural textures, their applicability for forensic shoeprint images is questionable.
LBPs were not suggested for shoeprint identification by the publications reviewed in the previous sections.
Furthermore, the presented algorithms of this section focus on invariance against rotation, e. g. \cite{khellah2011texture}, scale, e.g. \cite{davarzani2015scale}  and illumination, e.g. \cite{zhang2017learning}.
This is a possible issue on natural texture images, but the samples of the FID-300 dataset are rotation corrected and, as discussed in the following chapter, distortions and additional noise are the main challenges.
For that reason the benefits of LBP are limited in the field of shoeprint matching.

\section{Summary}
\par
In this chapter state-of-the-arts method of the field of shoeprint image, fingerprint and natural image enhancement and description were reviewed as well as noise removal algorithms were discussed.
Since there is no common forensic shoeprint image database, an overview of the techniques proposed for shoeprint images and their performance according to the testing datasets is given.
To propose an effective enhancement algorithm for shoeprint images from FID-300, the applicability of the algorithms for real-life shoeprint images were also discussed. 
\par
Thresholding and bandpass filtering as well as image processing techniques in the frequency domain are used in every discussed field to enhance the input.
Furthermore, along local filtering for shoeprint and fingerprint images, non-local filtering have promising results on natural images.
In the following, three shoeprint enhancement methods are introduced based on the publications discussed above.

%\par
%In \cite{kannala2012bsif}, a new LBP similar descriptor is defined called Binary Statistical Image Feature (BSIF) which is also proposed to use by Crossier et al. \cite{crosier2010using}.
%In BSIF, pre-learnt filters are used and the responses are stored in the feature vector.
%It can handle large intra-class variance when used for classification, however, it varies greatly when the scale changes. 
%For this reason, Crossier et al. \cite{crosier2010using} suggest calculating the features on multiple resolutions in order to gain scale invariance as well.
%\par
%Varied Local Edge Pattern Descriptors (VLEP) \cite{yan2016edge} are used to represent edge information.
%Every pixel of an edge is described by the angle and by the magnitude of the gradient, which stand for edge direction and strength, respectively.
%Wang et al. \cite{wang2018using} extend the feature to be scale invariant by combining two or more modified VLEPs, one calculated on a different
 


\chapter{Methodology}

\par
In this chapter three algorithms for shoeprint enhancement are introduced.
Two ways for image enhancement are extracting the image content regardless the noise and suppressing the noise without demaging those parts of the input which is considered as content.
The first proposed method follows the first approach and utilize feature learning.
The second one uses noise suppression by image filtering techniques.
The third one combines the two approaches by obtaining the shoeprint by building  a Fourie-Mellin based noise model and suppressing noise by applying the non-local means algorithm.
As mentioned earlier the testing data of already published algorithms varies, thus it is difficult to make an estimation about their performance on a FID-300  \cite{kortylewski2014unsupervised} dataset.
For that reason three prototypical application are developed and evaluated in this thesis, and the one with most promising results is further analyzed.
The goal of this chapter is to give an overview about the proposed methodology and to present the working pipeline and the implementation details of the methods.
\par
A measure for image quality is the signal to noise ratio, a high value indicates high quality image, where the content is clearly visible, a lower value stands for low quality where the noise suppresses the content of the image \cite{moubark2016clutter}.
In case of forensic shoeprint images, the outlines of the shoeprint represent the content, the information of the image, and clutter or additional objects in the background are noise. 
Since there is no pixel-wise annotation available, the content, namely the foreground, and the noise are not always distinguishable especially in images with high noise component, see Figure \ref{fig:rw:lowFID} where the shoeprint is hardly recognizable and it is not clear which part of the shoe sole is visible.
The three proposed algorithms attempt to enhance the shoeprints by increasing the signal to noise ratio of the input, thus increasing the contrast between the fore- and the background of the image.
The first algorithm focuses on finding the shoeprint pattern in varying conditions, the second one filters noise and the last one combines both of them by suppressing the noise with the calculated noise model and enhancing the shoeprint using the non-local means method.

\section{Feature Learning for Shoeprint Detection}

\par
To find the shoeprint in a shoeprint image regardless the noise a robust and discriminative descriptor is defined.
Since the noise is highly variable in real-life settings, the shoeprints of the same shoe outsole  pattern differ depending on the circumstances, such as ground properties, weather etc., causing high intra-class variance.
Because of that, a discriminative feature learning algorithm is proposed based on the work of Guo et al. \cite{guo2012discriminative}.
Guo et al. \cite{guo2012discriminative} proposed an adaptive feature selection method to determine discriminative features for natural image textures.
While training multiple samples of the same texture captured in different circumstances are processed and the features occurring on every sample are summarized in the feature pool.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00009_mask.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025_mask.jpg}
  \end{subfigure}
  \caption{Examples from FID-300 \cite{kortylewski2014unsupervised} dataset chosen for feature learning and their corresponding manually annotated masks.}
  \label{fig:pe:mask}
\end{figure}

\par
The feature learning algorithm proposed in \cite{guo2012discriminative} was developed originally to find a discriminative feature set for several texture images from a wide database, from \cite{ojala2002outex}, \cite{dana1999reflectance}, \cite{boland2001neural}, \cite{jantzen2005pap} and \cite{brahnam2007introduction}.
But in this project no such dataset is available since only a subset of samples from FID-300 are usable for learning. 
The FID-300 \cite{kortylewski2014unsupervised} dataset consists of 300 real-life forensic shoeprint imaged and 1175 reference shoeprint images, thus not every reference image has a corresponding real image, and the majority of reference images with existing corresponding real images have only one or two examples for real shoeprint image in the database.
Since the feature learning algorithm requires multiple representations for the same texture, a shoe sole pattern with the highest amount of real-life shoeprint images is chosen as training set.
The training set thus contains six real-life shoeprint image and their reference image. 
Since there is no pixel wise labeled data in the FID-300 dataset available, the samples chosen for training were labeled manually by comparing the sample images to their reference and annotating all pixels which are deemed part of the shoeprint pattern.
Figure \ref{fig:pe:mask} shows two example images. 
The black regions of the mask show the shoeprint, whereas the white pixels indicate the background.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{3/workflow.jpg}
  \caption{The workflow of the three-layered feature learning algorithm  \cite{guo2012discriminative}.}
  \label{fig:pe:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
In the work of Guo et al. \cite{guo2012discriminative} a three-layered Local Binary Pattern (LBP) feature learning algorithm is introduced for natural texture description.
The schematic workflow of the algorithm is shown in Figure \ref{fig:pe:workflow}.
They utilize feature selection where only the discriminative descriptors are chosen and propagated further.
In the first layer the samples of the same texture are examined separately.
Robustness is granted by selecting those features that describe the majority of a texture of the given sample.
This is done by finding the frequently appearing descriptors and ignoring the rare ones, since they are more sensitive to noise.
This learning process assumes high quality data, where the content, in this case the depiction of the given texture, dominates the samples and noise occurs on a smaller part of the image.
This is however not necessarily the case for the shoeprint images from FID-300, for that reason the learning process is altered which is described shortly.
The descriptors of the given sample are selected in the order of occurrences, starting with the most frequent one.
If the already selected descriptors cover a bigger region of the original texture than a given threshold, the selection process terminates and the chosen descriptors are propagated for the next level.


\par
Guo et al. \cite{guo2012discriminative} propose their algorithm for LBP features, and since it is a popular descriptor for natural textures \cite{hong2014combining}, \cite{ahonen2009rotation} and fingerprints \cite{wang2013pixel}, \cite{rida2018palmprint}, LBP is chosen to be a candidate descriptor for shoeprint detection.
In the research of shoeprint identification both frequency based feature descriptors, such as Fourier-Mellin Transform \cite{wu2019crime}, \cite{gueham2008automatic}, and point descriptors, such as SIFT \cite{nibouche2009rotation}, \cite{richetelli2017classification}, were already proposed, thus these are also selected for feature learning.
Therefore along LBP, the feature learning algorithm is also implemented for Fourier-Mellin and SIFT, but the selection process follows the same steps as proposed by Guo et al. \cite{guo2012discriminative}.
For the calculation of Fourier-Mellin descriptors the image is extended by mirroring the edges to have a uniform-sized, 5x5, descriptor in all cases.
For LBP features two different settings are considered, one with a radius of 3 and 12 sample points and one with a radius of 5 and 24 sample points.
These parameter settings were determined experimentally and their performance is discussed in the following chapter.
\par
 To determine the frequency of the descriptors, a similarity measure is used for all three candidate features. 
As similarity measure histogram correlation is used for LBP, a matcher, called Brute-Force, is applied for SIFT and the Fourier-Mellin features are evaluated using the correlation coefficient proposed  in \cite{gueham2008automatic}.
Brute-Force compares a given feature to every other features of the image using the Euclidean distance and sets the closest one as match storing the distance value between them.
The correlation coefficient proposed by Gueham et al. \cite{gueham2008automatic} is given as follows:
\begin{equation}
corr(FM_{1},FM_{2}) = \frac{\sum\limits_{m}\sum\limits_{n}(FM_{1mn}-\overline{FM_{1}})(FM_{2mn}-\overline{FM_{2}})}{\sqrt{(\sum\limits_{m}\sum\limits_{n}(FM_{1mn}-\overline{FM_{1}})^2)(\sum\limits_{m}\sum\limits_{n}(FM_{2mn}-\overline{FM_{2}})^2)}}  
\label{FMcorr}
\end{equation}
where $FM_{1}$ and $FM_{2}$ are two Fourier-Mellin Features to compare, $\overline{FM_{1}}$ and $\overline{FM_{2}}$ are the mean values of the corresponding features and $m$ and $n$ represent the size of the area the Fourier-Mellin features were extracted from.
The higher the correlation coefficient, the more similar the two compared descriptors are, 1 indicates complete match 0 stands for no correlation.
If the similarity value reaches a predefined threshold, the two descriptors are considered as same, and the counter for the frequency of the given descriptor is increased.
The experimentally determined threshold for similarity is  90\% for LBP histogram correlation, 300 for the maximal distance of two SIFT features and higher correlation than 0.95 between two Fourier-Mellin descriptors.
\par
Guo et al. \cite{guo2012discriminative} suggest a threshold based on the area the selected features cover on the original image.
The selection of threshold is crucial; if it is too high, less robust descriptors with low frequency are also selected, when the threshold is too low, only the most frequent descriptors are considered and valuable details of the texture are ignored.
The parameter settings are determined experimentally and every Fourier-Mellin feature is propagated which occures at least 10 times on the shoeprint pattern.
However, this criteria is altered in the case of LBP and SIFT to eliminate by noise distorted descriptors as follows.
Features are firstly extracted both from the fore- and the background of the samples, where the foreground contains the annotated shoeprint and the background is the noisy ground where the shoeprint is lying. 
After this, to achieve high inter-class distance between shoeprint and noise, frequent descriptors of the noise are determined and all of them are eliminated from the foreground descriptors.
The same similarity measures defined above are utilized to determine the frequency of the noise descriptors and to eliminate them from the foreground descriptors.
If the histogram correlation between two LBP noise features is higher than 90\% or the matching distance between two SIFT descriptors is lower than 300 the counter for frequency of the given noise feature is increased.
These thresholds and the following parameter setting are determined experimentally and the noise similarity criteria is set to be the same as the required similarity between descriptors of the shoeprint.
Afterwards the most frequent noise descriptors are determined, occurring at least 100 times among LBP and at least 10 times among SIFT features.
All LBP and SIFT shoeprint descriptors are eliminated which have higher than 90\% histogram correlation with or have smaller than 250 distance to any dominant noise descriptor.
The remaining shoeprint descriptors are propagated to the next level. 
\par
The following two layers of the original algorithm proposed by Guo et al. \cite{guo2012discriminative}  work according to Fisher's Separation Criteria.
The second layer aims to minimize intra-class variance, whereas the third layer ensures maximum inter-class distance. 
The second layer gathers all selected descriptors from the previous layer of the same texture, that are the extracted shoeprint features, from different images and intersects those sets.
The assumption is that shoeprints of the same shoe outsole pattern have similar properties under varying circumstances.
During this process, features are selected that occur in every shoeprint image of the given shoe outsole pattern.
If there is a feature in every shoeprint image sample of the same shoe outsole pattern with a similarity higher than a given threshold the feature is chosen and added to the final descriptor pool.
The same similarity measures defined in the first layer are used for feature comparison.
The thresholds for similarity are determined experimentally.
If the correlation between two LBP descriptor is higher than 90\%, the distance between two SIFT features is smaller than 450 and the correlation between two Fourier-Mellin features is higher than 0.95, the feature is chosen and added to the final set of descriptors.
\par
In the last, third layer representation capability is ensured.
This layer merges all remaining features from the second layer into one set to create a dominant feature pool.
In the second layer a feature set for a given texture is created based on the extracted descriptors from all samples of the same texture.
In the third layer those feature sets of every occurring texture are collected and united.
For the purpose of this prototypical implementation only the first two layers are used, because only one texture class, "shoeprint", is defined.
Since the noise pattern is unique to each sample and seemingly does not follow any regularities across them, descriptors for shoe patterns are only learned.
That means, that only one texture set is available, therefore no merging across multiple textures is needed.
\par
At the end of the learning process a descriptor pool is created.
The shoeprint area of the input to be tested is determined based on those learnt features using the same comparison techniques as in the learning phase.
The descriptors of the input image are determined similarly as in the training phase.
For every pixel of the input a descriptor is calculated.
The output image is calculated by comparing the descriptors of the input with the ones in the descriptor pool.
In case of LBP and Fourier-Mellin features the correlation value is written into the resulting image which is normalized at the end of calculation.
The result image of the SIFT feature comparison is binary, it is set to one if the descriptor of the corresponding pixel has a smaller distance than 200 to any descriptor from the learned set.
This distance is determined experimentally, its influence on the resulting image is discussed in the following chapter.


\section{Fully-Automated Noise Elimination}

\par
Similar to the previous method the main goal is to distinguish between noise and pattern, but this time instead of finding pattern regardless of the noise, the noise is suppressed in two steps, in the background first, and on the shoeprint afterwards.
The method proposed in this chapter has three main steps, first the noise is suppressed on the entire image, after that the shoeprint is enhanced and the resulting image is generated lastly by thresholding.
A similar pipeline, namely to filter the shoeprint image first and to apply thresholding afterwards, is  often used as a preprocessing step for shoeprint matching \cite{alizadeh2017automatic}, \cite{wang2014automatic}, \cite{li2014retrieval}, \cite{kong2014novel}.
Katireddy et al. \cite{katireddy2017novel} propose to enhance shoeprint impressions using Successive Mean Quantization Transform (SMQT).
In the presented algorithm those two approaches are combined by adding an enhancement step between filtering and thresholding.
In this way the noise is suppressed first, so that SMQT affects the relevant, shoeprint, area, and the pipeline is closed with thresholding to generate the binary image and to eliminate remaining noise parts.
The workflow of the entire algorithm is shown on Figure \ref{fig:fans:workflow}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{4/flow.jpg}
  \caption{The workflow of the fully automated noise suppression algorithm.}
  \label{fig:fans:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
The first part of the algorithm is responsible for noise suppression.
Two filters proven to be effective in natural image noise elimination are combined, these are Wiener filter \cite{li2014rapid}, \cite{chatterjee2011patch} and Bilateral filter \cite{zhang2016simultaneous}, \cite{huang2013self}.
As Li et al. \cite{li2014rapid} state, Wiener filter is among the most popular techniques for noise reduction.
Wiener filter works in the signal domain, where it estimates the original image based on the cluttered input, in this case based on the noisy real-life shoeprint image. 
The filter is given by following Equation \cite{Win}:
\begin{equation}
G(u, v) = \frac{H^*(u,v) P_s(u, v)}{\mid H(u,v)\mid ^2 P_s (u, v) + P_n (u, v)}  
\end{equation}
where $H(u,v)$ stands for the Fourier Transform of the point-spread function, $P_s (u,v)$ denotes the power transform of the signal, which is the Fourier Transform of the signal autocorrelation and $P_n(u,v)$ expresses the power spectrum of noise, which is the Fourier Transform of the noise autocorrelation.
 $P_s(u,v)$ and  $P_n(u,v)$ essentially represent the two components of the image, the content and the noise, in the signal domain.
The point-spread function defines the appearance of a point-like object on the images.
The performance of the filter depends on the quality of $P_s$, on the estimated appearance of the original image.
In this method the estimation is made based on the entire input image, because no further information, for example a mask about the shoeprint  area, is available.
However, this also means that the estimation is less accurate than using a mask on of the shoeprint area, thus the performance is lower \cite{chatterjee2011patch}.
To apply the Wiener Filter the input image is normalized first to the range of 0 to 1.
For the Point Spread Function an image of size 5x5 is set.
The balance parameter is 1100 which sets the ratio between information adequacy and prior adequacy, those parameters control frequency increment and decrement respectively.
The parameter settings are determined experimentally and set a smaller kernel size, thus less aggressive filtering, intentionally.
As mentioned above the estimation of the appearance of original image is limited, since the original shoeprint image without noise is not available.
Therefore it is possible that apart from suppressing noise, the shoeprint is also damaged.
With smaller kernel size the noise is less blurred but in exchange higher frequencies such as the shoperint is preserved.
Similar to the Wiener Filter, Bilateral Filter is also used for smoothing the image.
Since it considers the weighted average of neighboring pixels, it preserves the edge information which is crucial to prevent blurring on the shoeprint \cite{elad2002origin}. 
The kernel size of the Bilateral Filter is 5x5 which was determined experimentally and is based on the same principle as the parameter settings of the Wiener filter.
A bigger kernel size results in more blurred image background, however, it is not able to preserve the fine-lined foreground area, since the bigger neighborhood outweights the small lines occurring in the filter window.
For that reason a smaller kernel is chosen, to avoid the possible blurring on the shoeprint.
When both images are calculated the difference between them is propagated.
In this way the blurring effect is strengthened in the background while the outlines of the shoeprint patterns are preserved.
With the combination of both filtered images, the effect of the Wiener and Bilateral filters is amplified, the little blurring caused by the small filter kernels is added in the background while the foreground stays undamaged.
\par
For enhancement Successive Mean Quantization Transform (SMQT) is used \cite{nilsson2013smqt}.
The method was already proposed for shoeprint enhancement by Katireddy et al. \cite{katireddy2017novel}.
SMQT is a recursive algorithm working on subsets of the entire data which splits the given set into two parts in every recursion level depending on the current value being smaller or bigger than the mean of the entire subset.
In the first recursion level the data, in this case the shoeprint image, is separated into two parts, and it is noted which pixel is under (0) and which one is above (1) the current mean.
The algorithm then recursively continues on both subgroups of the image, splitting and noting the relative value compared to the mean of the subgroup  for every pixel.
Note that the pixels in the same subgroup does not have to be neighboring, the clustering is solely based on the pixel value.
The recursion stops if the predefined depth is achieved.
To finish the transformation the noted relative values of the pixels are examined.
Along the levels of the recursion a sequence of ones and zeros are registered for every pixel.
As a last step, this sequence is considered as a binary number and its decimal value is written into the corresponding pixel.
SMQT works based on the similar principle as Histogram Equalization.
Because of the successive partitioning, the pixel values are spread on the range of the depth of the recursion.
In this way the structure of the data is exposed.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.09\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.9\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/table.jpg}
  \end{subfigure}
  \caption{The first 8 columns of the occurrence map. The header of the table represents the possible pixel values }
  \label{fig:fans:table} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}
\par
Because of time efficiency, a speeded up version is implemented.
In the first step a table of occuring values on the image is created where the zeroth column corresponds the pixel value of 0, the first column stands for pixel value of 1 and so on.
In the first row of the table, the frequency of the given value is noted, that is the number of pixels having the value which the column corresponds with.
In the second row the sum of frequencies up to the current column is stored.
The third row represents the sum of all elements until the given column.
The SMQT algorithm is then applied on this table instead on the input image.
Since the entries are ordered, if the subgrouping starts, the values belonging to the same cluster will be neighboring columns.
Figure \ref{fig:fans:table} shows the first eight columns of the occurrence map of an example image.
This table eases the calculation of mean value for every level of the SMQT algorithm.
For the mean calculation the sum of all pixels is divided by the number of pixels, which is stored in the third and second row of the table respectively. 
To calculate the mean value of one subgroup two columns of the table are considered, the one before the first column of the subgroup and the last one in that given group.
The values are first corrected by substracting the elements of the former from the latter, and then dividing the third row with the second one the average value of the group is determined.
This correction is needed to eliminate the offset in the table, skipping this step the mean of every pixels having lower or equal value than the biggest element of the given subgroup is calculated.
When the occurrence map is ready the recursion starts, the mean of the given subgroup is determined and the occurrence map is split into two parts according to the pixel value of the given column being bigger or smaller than the mean value. 
Every column of the table has a binary code which is created during the recursion.
In the first level the first digit of the code is written, in the second level the second digit etc., the length of this code is the same as the predefined depth of the recursion.
If a pixel value is smaller or equal to the calculated mean of the given subgroup a zero is written into the belonging binary code, and a 1 is recorded if it is bigger.
The manually set depth of the recursion is 8, in this way when the final binary codes are converted to a decimal number a range of 0 to 255 is covered.

\par
The last part of the application is the postprocessing step where the input is converted into a binary image and inconsistencies and remaining noise are eliminated.
When binarizing footprint images Otsu's technique \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{wu2019crime} or adaptive thresholding \cite{wang2014automatic} is used frequently.
However in the proposed approach a local thresholding technique, called Niblack Binarization Method (NBM) \cite{niblack1985introduction} is preferred.
There are publications available \cite{som2011application}, which prove that global methods, such as Otsu Thresholding \cite{otsu1979threshold}, is less feasible as their local counterparts.
Although the studies mentioned were carried out on text documents with varying image quality, the two domains are considered familiar since both aim to find fine line structures on a cluttered background.
Furthermore, Saxena et al. \cite{saxena2019niblack} also state that NBM is one of the most powerful thresholding methods, outperforming the global and some local techniques as well. 
NBM  is calculated as follows \cite{saxena2019niblack}:
\begin{equation}
T_d = m(x,y) + k * s(k, y)
\end{equation}
where $m$ and $s$ stands for mean and standard deviation in the given area respectively and $k$ is a configuration variable which is given manually. 
In this thesis the window size is the same as the size of the Wiener Filter, 5x5, and the regularization parameter $k$ is set to 0.5 which values were determined experimentally.
\par
There is, however, one disadvantage of local binarization and that is the  local window size.
Since for every subwindow a new threshold is calculated NBM tends to generate salt and pepper noise on more homogeneous, e.g. background of the shoeprint, area.
For that reason two other postprocessing methods are also implemented based on the connected components of the image.
They are extracted considering 8-neighborhood, so diagonal connectivity is also considered.
The first postprocessing step eliminates short lines on the image, based on the assumption that the outlines of the shoe pattern build longer, coherent edges.
The second one deletes all remaining open structures unless the length is higher than a given threshold.
This value is set to be higher than the threshold in the previous step.
Those two heuristics are based on two observations, first,  when there is no shoepint impression region in a NBM subwindow the edges of the remaining clutter and binarization artifacts are generated.
Second, since NBM concentrates on the contours, if the complete structure of a pattern element is found a closed line structure is extracted.
It is possible, however, that an open line structure is generated,  if only parts of the shoeprint are visible or identified correctly.
For that reason, only a subgroup of open lines, the ones that are shorter than a threshold, is eliminated.
It is possible to set the thresholds of the two postprocessing steps to the average size of every region, however, experimental results show that such strong criterion eliminates shoe pattern lines as well.
For that reason two experimentally defined, manually set thresholds are used, they are set to 50 and 60 respectively for every test image.
If an open line structure is found which overall size is smaller than 60, it is deleted.

\section{Shoeprint Enhancement with Non-Local Means}
\par
Since noise tends to appear on the entire shoeprint image not only in the background region, a semi-automated algorithm is introduced in this section, where user input about the noise is expected and based on that information a noise model is calculated.
As mentioned earlier filtering the noise first and using a thresholding method afterwards is widely used as preprocessing step for shoeprint matching \cite{alizadeh2017automatic}, \cite{wang2014automatic}, \cite{li2014retrieval}, \cite{kong2014novel}.
Similar to that pipeline, the proposed algorithm has two main steps.
First, the noise is eliminated in two stages.
In the first step the background and the foreground, the assumed area of the shoeprint, is separated to mask the former region.
After that, the the foreground is filtered in the frequency domain to suppress  the noise covering that area.
Second, the shoeprint pattern is enhanced, by extracting the edges and applying the non-local means method, and binarized finally.
A full separation between fore- and background is possible, because according to the user input a detailed background and noise model is built.
In this way, it is possible to mask the noise in the calculated background area instead of just suppressing it with filtering.
Furthermore, the masking does not modify the foreground area which is not guaranteed when using a filter on the whole input sample.
As a first step of algorithm, the user has to select a pixel in the backgroun where there is no pattern information, no foreground, in the neighboring area.
The full pipeline is shown in Figure \ref{fig:sans:workflow}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{5/flow.jpg}
  \caption{The workflow of the semi-automated noise suppression algorithm.}
  \label{fig:sans:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Based on the user input the background area is determined according to the Fourier-Mellin features \cite{sheng1986circular}.
Fourier-Mellin features were already used for shoeprint description for both synthetic \cite{gueham2008automatic} and real \cite{wu2019crime} shoeprint images, however, in this algorithm they are used for noise description.
This decision is based on the observation that although the noise varies among samples, there are dominant features within one image which describe the clutter in the shoeprint images.
The varying appearance of noise is illustrated in Figure \ref{fig:sans:noiseIll}, comparing the background of the images there is little or no similarity across the samples, however, within one shoeprint image dominant patterns are recognizable.
To prepare the descriptor for the variance of the noise, three, different-sized samples are extracted around the point determined by a user input.

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noiseIll/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noiseIll/00017.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noiseIll/00025.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noiseIll/00204.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noiseIll/00239.jpg}
  \end{subfigure}
\caption{Example images from the FID-300 dataset. The noise varies across the images, but there are dominant structures within one sample.}
\label{fig:sans:noiseIll}

\end{figure}
\par
Fourier-Mellin features are scale, rotation and translation invariant descriptors and are calculated according to the following equation \cite{kazik2011visual}:
\begin{equation}
\mathcal{M}_f(u,v) = \frac{1}{2\pi} \int_{0}^{\infty}\int_{0}^{2\pi} f(r, \theta)r^{-ju}e^{-jv\theta}\mathrm{d}\theta\frac{\mathrm{d}r}{r}
\end{equation}
where $u$ and $v$ are the Mellin and the Fourier transform parameters respectively.
If the image is translated, the Fourier-Mellin transformed representation does not change.
That is however not the case when the image is scaled or rotated.
In both scenarios phase shift appears on the feature descriptor and the magnitude changes according to the scaling factor.
Because of those reasons above, the feature descriptor is converted to Log-Polar coordinates, so that image transformations occur as a translation in the descriptor.
Log-Polar Transform is used to achieve robustness against scale and rotation  \cite{gueham2008automatic}.
The Log-Polar coordinates of a point with Cartesian coordinates are given by \cite{sarvaiya2012image}:
\begin{equation}
(\rho,\theta) = (\sqrt{(x-x_c)^2 - (y-y_c)^2}, \tan^{-1}\frac{y-y_c}{x-x_c})
\end{equation}
where $x$ and $y$ are the Cartesian coordinates of the point and $x_c$ and $y_c$ are the center coordinates of the input image.
The Log-Polar coordinates represent the radial distance, $\rho$, and the angle from the center, $\theta$.
This conversion is illustrated in Figure \ref{fig:sans:logPol} \cite{sarvaiya2012image}.
In the left side of the image the Log-Polar, in the right side the Cartesian coordinates are shown.
x is converted to the distance from the origin, whereas y stands for the angle in counterclockwise order.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{5/logPol.jpg}
  \caption{Illustration of the relation between Log-Polar and Cartesian coordinates \cite{sarvaiya2012image}}
  \label{fig:sans:logPol} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
The Fourier-Mellin descriptors extracted from the three different sized neighborhoods around the user input represent the noise, after that every pixel of the input sample is examined for correspondence with the metric defined in the first section of this chapter \ref{FMcorr}.
The diameter of the neighborhoods is determined experimentally and is set to 6, 12 and 18 respectively.
The borders of the shoeprint image are extended by mirroring additionally to ensure that the calculated Fourier-Mellin features have uniform size for every pixel on the image.
The calculated similarities are stored in a correlation map by taking the average correspondence value of the three different sized subimages, and the correlation map is binarized by Otsu's method afterwards.
This thresholding method was used for two reasons.
First, it is popular in the forensic image processing for all three kinds of datasets, i.e. at synthetic \cite{algarni2008novel}, \cite{alizadeh2017automatic}, restricted \cite{kong2014novel} and also at real forensic samples \cite{wu2019crime}.
Second, with a global thresholding method the pivot value between fore- and background pixels are found.
To complete the background mask, morphological operations are applied, using an experimentally defined, 8x8 kernel, Opening first and Closing second, to eliminate small holes and rough contours as proposed by many approaches for shoeprint identification \cite{wang2014automatic}, \cite{kong2014novel}, \cite{li2014retrieval}, \cite{tang2010footwear}.
\par
Once the background mask is generated the foreground is processed to suppress the noise and enhance the shoeprint.
The mask eliminates the noise in the background area, however, it does not modify the remaining region, thus this area is examined next.
Noise on the shoeprint is suppressed first, afterwards the edge information is extracted and the binary image is generated lastly.
The noise is possibly located on the shoeprint itself, to preserve the underlying shoeprint information frequency based noise suppression is applied.
Wavelet transform is often used for natural image denoising \cite{xu2016image}, \cite{sugamya2016image}, for fingerprint \cite{li2012texture} and for shoeprint enhancement \cite{katireddy2017novel} as well.
During wavelet transform the image is decomposed into subbands, and the subimages are processed separately.
In case of four subbands, the LL image contains the edge information and the other ones the intensity values.
Depending on the properties of the noise the image clutter is separated from the information and suppressed on the corresponding sub-band.
After that the image is reconstructed by inverting the decomposition process.
\par
The same principle is used in this approach, but instead of wavelet transform, Fourier transform is applied, to further exploit the previous step of the algorithm.
The Fourier representation of every pixel on the shoeprint sample is calculated using the Discrete Fourier Transform on a 6x6 window, the smallest neighborhood used in the previous step.
This time the window size is fixed, since the final model is based on the neighborhood of every background pixel and not only on the area around the user input.
When the background is separated two models of the appearance of the noise are built.
The average noise model is calculated by taking the mean descriptor of the Fourier representations of the background pixels. 
The other, centorid model is determined by calculating the centroids of the k-means algorithm applied on the background area.
The k-means algorithm is set to terminate after 10 iterations, whereas k equals to 8, so 8 centroids are determined which are the representations of the noise on the given shoeprint image.
These parameters were set experimentally by focusing on the robustness and versatility of the submodels.
Increasing k, more noise-models are calculated, thus the noise description is more versatile.
But in the same time, the single models, the centroids, are based on a smaller set of pixels compared to the case when k is smaller, therefore they are sensitive for outliers as well. 
The noise is suppressed by calculating the difference between the noise model and the Fourier representations of the foreground pixels.
In case of the averaged noise model, the Fourier image of noise is subtracted from the Fourier image of the foreground pixel and its neighborhood and the image is reconstructed using the Inverse Fourier Transform.
When using the centroid noise models, the Fourier image of the foreground pixel is compared to all possible submodels, and the one with highest correlation is used.
The correlation coefficient is determined using the Equation for Fourier-Mellin Feature comparison \ref{FMcorr}.
When the centroid is decided the difference between both Fourier representations is calculated and the foreground is reconstructed again using the Inverse Fourier Transform.
The performance and the advantages of both noise representations are discussed in the following chapter.
If the background area is smaller than a given threshold, this step is skipped, because in this case no robust model about the noise can be made.
In this case it is set to 20\% of the image which was determined experimentally.
\par
For line extraction and shoeprint enhancement a gradient based method published by Zeng et al. \cite{zeng2011region} is used.
They propose a region based non-local means algorithm to denoise natural images.
Non-local means was developed for image denoising and grants better smoothing while preserving the details on the image than local mean calculation \cite{buades2005non}.
This is achieved by the averaging pixels with similar properties and not the ones within the same neighborhood.
In non-local means the image is separated into patches which are clustered according to a given criteria.
While smoothing the average value of the patches within the same class is calculated and the members are updated with that value.
Zeng et al. \cite{zeng2011region} propose to use the gradient information of the pixels for classification, which is ideal for shoeprint images, because the goal is to preserve the shoeprint edges during the enhancement.
Since the classification is made based on the gradients, edge properties are the essential clustering criteria.
Assuming that both the shoeprint and the noise on the shoeprint image have common properties which are different to each other, pattern and noise classes are created at the end of the custering.
In this way, the outlines of the shoeprint are preserved and with the non-local menas algorithm the residual noise is smoothed.
\par 
The pixels are clustered according to the eigenvalues of the tensor matrix given by \cite{zeng2011region}:
\begin{equation}
T_\sigma = 
\begin{pmatrix}
t_{11} & t_{12} \\
t_{12} & t_{22}
\end{pmatrix}
=
\begin{pmatrix}
G_\sigma*(g_x(i,j))^2 & G_{\sigma}*g_x(i, j)g_y(i, j)\\
G_{\sigma}*g_y(i, j)g_x(i, j) & G_{\sigma}*(g_y(i, j))^2
\end{pmatrix}
\end{equation}

where $g_x$ and $g_y$ represent the gradient information in the corresponding directions and $G_\sigma$ is the Gaussian kernel having $\sigma$ as standard deviation.
To do so, the image is blurred first with the Gaussian kernel of experimentally defined size of 11x11 to make the calculation more robust.
To extract the gradients the Sobel filter is used in x and y direction separately.
The Eigenvalues of $T_\sigma$ are then given by \cite{zeng2011region}:
\begin{equation}
\lambda_1 = \frac{1}{2}(t_{11} + t_{22} + \sqrt{(t_{11}-t_{22})^2 + 4t_{12}^2});
\lambda_2 = \frac{1}{2}(t_{11} + t_{22} - \sqrt{(t_{11}-t_{22})^2 + 4t_{12}^2})
\label{eig}
\end{equation}

The classification is based on the difference between the two Eigenvalues of the given pixel.
If the difference is small, it indicates smooth region, whereas a higher value implies edge area.
The proposed classification strategy is the following \cite{zeng2011region}:
\begin{equation}
(i, j)\in \left\{
                \begin{array}{ll}
                  c_1, if \lambda(i,j) \leq \lambda_{min} + \frac{1(\lambda_{max} - \lambda_{min})}{n}\\
                  c_2, if \lambda(i,j) \leq \lambda_{min} + \frac{2(\lambda_{max} - \lambda_{min})}{n}\\
				... \\
                   c_n, if \lambda(i,j) \leq \lambda_{min} + \frac{n(\lambda_{max} - \lambda_{min})}{n}
                \end{array}
              \right.
\end{equation}

where $\lambda_{min}$ and $\lambda_{max}$ stands for the minimum and maximum difference on the entire image respectively.
In this thesis two parameter settings are implemented using 25 and 20 classes, which were determined experimentally.
Their comparison and the discussion about their performance are given in the following chapter.
For denoising, the mean illumination value of pixels in the same cluster is calculated and the original pixel value is replaced with the average of the cluster.
Zeng et al. \cite{zeng2011region} propose an additional weighting scheme for mean calculation as well, so that pixels in the same class are weighted according to the geometrical distance between them and the given pixel to calculate.
However, the weighting method is skipped in this implementation, since geometrical location is not relevant in this use-case for two reasons.
First, the size of the region the image was taken of is generally significantly smaller than the scene on natural images.
Second, shoeprint images depict a 2 dimensional object, namely the shoesole impression of a given shoe on a given surface. 
Since there is no depth difference within the image, the entire area is considered as one connected region, unlike in natural images where the objects in the foreground and in the background do not belong together.
But another criteria is added to the clustering algorithm.
After determining the classes, if one or more cluster have more members than a given threshold, histogram equalization is applied on the image where the differences of eigenvalues are stored, and the classes are recalculated.
This threshold is set to be 60\% experimentally.
This optimization is needed to lower the chance that pattern and noise edges are assigned to the same cluster. 
\par
To find  the cluster with the noise edges, a simple heuristic is defined.
Since the clustering algorithm is applied on the entire image, not only on the foreground, the cluster with the highest amount of members is eliminated.
This decision is based on the assumption that noise is found in the entire shoeprint image, whereas shoeprint edges are located only at given areas.
Furthermore, it is also assumed that unlike shoeprint contours, where the edge properties change according to the original shoeprint structures and pressure, noise have similar appearance throughout the the entire sample.
In case of using Histogram Equalization, however, the class memberships are more balanced, so no such heuristic is assumed.
\par
In the next step, the image is binarized with the adaptive thresholding technique \cite{laine1996multiscale} popular in both fields including shoeprint identification  \cite{wang2014automatic}, \cite{li2014retrieval} and natural image processing \cite{xu2016image}.
Adaptive thresholding is a local binarization technique similar to NBM.
It generates multiple thresholds for every subregion of an image based on the Gaussian weighted sum in the window, which is experimentally set to 12 in this implementation.
\par
The closing step is postprocessing, where small discontinuities and remaining noise are eliminated.
Even though in the previous step noise edges belonging to the biggest class were eliminated, other noise edges assigned to other clusters are still part of the image.
This problem is solved with short line elimination.
To eliminate noise while preserving the shoeprint edges, the same assumption is made as the one in the previous section.
The noise consists of cluttered, small components, whereas the shoeprint is built of bigger, connected structures, thus the remaining small regions, smaller than a given threshold, are eliminated.
The connected components of the image using 8-connectivity are extracted and every element with an area not bigger than 15 is deleted immediately.
This threshold is set manually and is significantly smaller than those in the previous section deleting every structure smaller than 50 or open structures smaller than 60.
This time the majority of noise is already eliminated, since the background is masked, and adaptive thresholding, unlike NBM, does not generate thresholding artifacts and noise clutter.
Because only the residual noise around the outlines of the shoeprint has to be deleted, no such high threshold for small structure elimination is set.
\section{Summary}
\par
In this chapter the methodology of three algorithms for shoeprint enhancement were introduced and described.
Two approaches for enhancement of an image are content extraction and noise suppression.
The first presented algorithm is based on feature learning, to define robust descriptors for shoeprints.
During training, common features of the pixel-wise annotated shoeprint images are selected to create a descriptor pool.
When processing the input, the similarity between its features and the feature pool is examined.
The second algorithm utilize on noise suppression where no previous knowledge about the database and the input image is utilized.
The noise is suppressed in two steps, Wiener and Bilateral filter is used first to eliminate noise without damaging the edge information.
Afterwards SMQT is applied.
In this way the structure of the shoeprint image is revealed and the input is prepared for thresholding.
For binarization NBM is used, and to eliminate the line clutter caused by local thresholding, heuristics about the size of relevant edgese are defined.
The lastly proposed algorithm combine the two approaches.
Instead of the shoeprint model learned in the first algorithm, a model about the noise in the input image is calculated.
To do so, user input is required.
Based on the calculated noise model the back- and the foreground of the shoeprint image are separated, whereas the foreground area is deemed to contain the shoeprint.
The foreground is corrected according to the noise model to suppress the noise and further processed applying the non-local means algorithm.
The resulting image is finally binarized by adaptive thresholding.
In the next chapter, the evaluation of the algorithms on FID-300 shoeprint images is presented. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Discussion}
\label{results}
\par
In this chapter the experimental results of the algorithms presented are shown, and it is discussed if they are usable for enhancement of real-life shoeprint images from the FID-300 dataset.
Along the qualitative evaluation of all algorithms, the quantitative evaluation of the most promising one is also given.
All algorithms are implemented in Python 2.7 \cite{van1995python} using OpenCV 4.1.1 \cite{opencv_library}.

\section{Qualitative Evaluation}
In this section the qualitative evaluation of the three proposed algorithms is presented.
Based on the results, their advantages and disadvantages are examined and possible ways for improvement are discussed.  
\subsection{Feature Learning Algorithm}
\par
Two kinds of experiments were conducted to test the performance of the descriptor.
Testing on the training set and testing on an evaluation set, on shoeprint images which were not considered while training.
The first scenario is discussed now, and the results of the second one afterwards.
Tests are performed on the training set to see how many descriptors from the original image were eliminated, and to identify the common features in the training dataset.
Figure \ref{fig:pe:25} and Figure \ref{fig:pe:66} show example results of this testing setup.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025.jpg}
    \subcaption{}
    \label{fig:pe:25:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00025.jpg}
    \subcaption{}
    \label{fig:pe:25:LBPs}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00025_6_24_5.jpg}
    \subcaption{}
    \label{fig:pe:25:LBPb}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00025.jpg}
    \subcaption{}
    \label{fig:pe:25:FM}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00025.jpg}
    \subcaption{}
    \label{fig:pe:25:SIFT}
  \end{subfigure}
  \caption{Output of the feature learning algorithm on an image from training set.\subref{fig:pe:25:orig} Example image from the training set \subref{fig:pe:25:LBPs} Output using LBP descriptors with radius 3 and 12 sample points \subref{fig:pe:25:LBPb} Output using LBP descriptors with radius 5 and 24 sample points \subref{fig:pe:25:FM} Output using Fourier-Mellin Descriptor \subref{fig:pe:25:SIFT} Output using SIFT Descriptor}
  \label{fig:pe:25}
\end{figure}

\par
The visible difference between Figure \ref{fig:pe:25:LBPs} and Figure \ref{fig:pe:25:LBPb} as well as between Figure \ref{fig:pe:66:LBPs} and Figure \ref{fig:pe:66:LBPb} shows that bigger neighborhood of LBP, in this case radius of 5 and 3 are considered, is a better choice for shoeprint description.
Considering the middle region of Figure \ref{fig:pe:25}, the background in Figure \ref{fig:pe:25:LBPb} is less noisy than in Figure \ref{fig:pe:25:LBPs}.
This difference is more visible in  Figure \ref{fig:pe:66} where in Figure \ref{fig:pe:66:LBPs} no shoeprint can be recognized meanwhile in Figure \ref{fig:pe:66:LBPb} outlines of the pattern are visible.
This observation is explainable with the properties of the LBP descriptor: a small neighborhood is more suitable for detailed, high-frequency textures, however, it is also more sensitive to noise.
The shoe outsole pattern presented in Figures \ref{fig:pe:25}  and \ref{fig:pe:66} consists of big geometrical structures, and the input is noisy, thus big neighborhood is more suitable.
Comparing Figure \ref{fig:pe:25:LBPs} and Figure \ref{fig:pe:66:LBPs} the contrast between outlines of the shoeprint and the background on the first one is higher, since the original image, Figure \ref{fig:pe:25}, contains less noise than Figure \ref{fig:pe:66}.
Still focusing on the middle area of Figure \ref{fig:pe:25}, Figure \ref{fig:pe:25:SIFT} shows that the SIFT descriptor is similarly robust against noise in the background as the LBP descriptor.
SIFT is a robust feature descriptor, therefore it is not surprising that it successfully distinguishes between shoeprint and background in such a clear image like Figure \ref{fig:pe:25}.
However, examining Figure  \ref{fig:pe:25:FM} the Fourier-Mellin descriptor has more difficulties with the background area, where the middle regions are noisy unlike in  Figure \ref{fig:pe:25:LBPb} and in Figure \ref{fig:pe:25:SIFT}.
On the other hand, inspecting the bottom left side of the shoeprint, it is seen that the Fourier-Mellin features menaged to find the whole area of pattern, whereas only outlines are recognizable in Figure \ref{fig:pe:25:LBPb} and in Figure \ref{fig:pe:25:SIFT}.
That indicates that the Fourier-Mellin features are more detailed descriptors than LBP and SIFT. 
The top part of the shoeprint consist of blobs, whereas on the bottom thin lines are seen.
The LBP and SIFT features mistake those lines as noise, because LBP considers a bigger, radius of 5 is used instead of 3, neighborhood and the robustness of the SIFT descriptor prevents to correctly recognize those fine edges.
Fourier-Mellin is more sensitive for intensity changes, thus the detailed shoeprint parts are correctly recognized.
On the other hand, it also causes noise in the middle and in the top of the image.
The same phenomenon is seen in Figures  \ref{fig:pe:25:LBPs} and  \ref{fig:pe:25:LBPb}, where the bottom left regions are recognized on the image  with smaller neighborhood, having the radius setting of 3 instead of 5, see Figure \ref{fig:pe:25:LBPs}, but the background is homogeneous on the latter, see \ref{fig:pe:25:LBPb}. 
Based on these observations,  more robust descriptors, such as LBP with the bigger radius setting and SIFT, are sensitive for blob regions..
The features less robust against noise, such as Fourier-Mellin, find both kind of regions, fine-lines and blobs,  of a shoeprint.
In exchange, the background area stays noisy and further image processing is needed.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00066.jpg}
    \subcaption{}
    \label{fig:pe:66:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00066_4_12_3.jpg}
    \subcaption{}
    \label{fig:pe:66:LBPs}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00066_6_24_5.jpg}
    \subcaption{}
    \label{fig:pe:66:LBPb}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00066.jpg}
    \subcaption{}
    \label{fig:pe:66:FM}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00066.jpg}
    \subcaption{}
    \label{fig:pe:66:SIFT}
  \end{subfigure}
  \caption{Output of the feature learning algorithm on an image from training set. \subref{fig:pe:66:orig} Example image from the training set; \subref{fig:pe:66:LBPs} Output using LBP descriptors with radius 3 and 12 sample points; \subref{fig:pe:66:LBPb} Output using LBP descriptors with radius 5 and 24 sample points; \subref{fig:pe:66:FM} Output using Fourier-Mellin Descriptor; \subref{fig:pe:66:SIFT} Output using SIFT Descriptor}
  \label{fig:pe:66}
\end{figure}

\par
%This testing scenario shows that the feature learning algorithm is able to distinguish between fore- and background on clear shoeprint images.
In Figure \ref{fig:pe:66} the pattern is less recognizable than on the previous example and background pixels are labeled wrongly as pattern.
If the input image is noisy, as in Figure \ref{fig:pe:66}, the results are not clear, and LBP and SIFT outperform the Fourier-Mellin descriptors.
In Figure \ref{fig:pe:66:FM} the shoeprint is barely visible, whereas in Figures \ref{fig:pe:66:LBPb} and \ref{fig:pe:66:SIFT} outlines are recognizable.
This is explainable with the different training process for Fourier-Mellin and for the other two features.
The selection process for Fourier-Mellin features was the same as suggested by Guo et al. \cite{guo2012discriminative}.
However, this pipeline were modified for LBP and SIFT features where the descriptors of the annotated background area were also extracted and explicitly eliminated from the final feature pool.
The results prove, that this modification provide a more robust descriptor set than the originally proposed feature training steps.
\par
Comparing all results of the two images, the performance depends on the input quality.
A noisy input, see Figure \ref{fig:pe:66}, produces less exact results than a clear sample, see Figure \ref{fig:pe:25}.
Feature learning makes it possible to label, to indicate the shoeprint area and distinguish it from background, in low quality images as well,  see Figure \ref{fig:pe:66}, but in comparison to a high quality input, see Figure \ref{fig:pe:25}, the accuracy is lower.
That means that the feature learning algorithm does not completely overcome the negative influence of noise, but it makes the descriptors more robust against it.
\par
In the second testing scenario an evaluation image set is defined, which consist of non-training shoeprint images, to examine if the features learned on shoeprint images from one specific shoe outsole pattern are able to describe shoeprints of other shoes as well.
In Figure \ref{fig:pe:182} the results of the Feature Learning algorithm on a non-training sample are shown.
The results displayed in Figure \ref{fig:pe:182:LBPs} and in Figure \ref{fig:pe:182:LBPb} strengthens the observation that bigger, 5, neighborhood LBP outperforms the smaller, 3, radius descriptor.
In Figure \ref{fig:pe:LPBexp} further examples are shown, in Figures \ref{fig:pe:204:LBPb}, \ref{fig:pe:241:LBPb} the contrast between the shoeprint and background is big compared to Figures \ref{fig:pe:204:LBPs}, \ref{fig:pe:241:LBPs}, because of the different neighborhood settings.
Furthermore, examining the middle, noisy part of the images in all three cases a homogenous background is seen containing some falsely labeled pixels.
However, the descriptiveness of the LBP features is limited for shoeprint images.
The goal of enhancement is to increase the contrast between the shoeprint and the background.
That is, however, not the case, compared to the original images, the contrast is decreased.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00182.jpg}
    \subcaption{}
    \label{fig:pe:182:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00182_4_12_3.jpg}
    \subcaption{}
    \label{fig:pe:182:LBPs}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00182_6_24_5.jpg}
    \subcaption{}
    \label{fig:pe:182:LBPb}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00182.jpg}
    \subcaption{}
    \label{fig:pe:182:FM}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00182.jpg}
    \subcaption{}
    \label{fig:pe:182:SIFT}
  \end{subfigure}
  \caption{Output of the feature learning algorithm \subref{fig:pe:182:orig} Input image; \subref{fig:pe:182:LBPs} Output using LBP descriptors with radius 3 and 12 sample points; \subref{fig:pe:182:LBPb} Output using LBP descriptors with radius 5 and 24 sample points; \subref{fig:pe:182:FM} Output using Fourier-Mellin Descriptor; \subref{fig:pe:182:SIFT} Output using SIFT Descriptor}
  \label{fig:pe:182}
\end{figure}

\par
Observing the bottom part of the shoe in Figure \ref{fig:pe:182} a weakness of LBP and SIFT already discussed is noticeable.
On the original image there are small structures in the bottom area which are part of the shoeprint.
Similar to the bottom left part of Figure \ref{fig:pe:25}, the fine structures are partially recognized.
Comparing Figure \ref{fig:pe:182:FM} to Figure \ref{fig:pe:182:LBPb} and to Figure \ref{fig:pe:182:SIFT}, Fourier-Mellin outperforms LBP and SIFT again labeling the whole area and not only the outlines correctly.
The output of the Fourier-Mellin features, see Figure \ref{fig:pe:182:FM}, is more clear than in the previous tests, as seen in Figures \ref{fig:pe:25:FM} and \ref{fig:pe:66:FM}, whereas no such change is noticeable between LBP and SIFT features.
As mentioned earlier, unlike in the case of LBP and SIFT training, the noise descriptors were not eliminated from the feature pool of Fourier-Mellin descriptors.
The tests on the training dataset indicated that, by producing less clear images than LBP and SIFT.
However, the evaluation set contains unknown noise which were not part of the training set.
Therefore, the similar shoeprint elements are recognized and the background is more distinguishable than in the previous cases.

\begin{figure}[h]
  \centering
	\begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/00204.jpg}
    \subcaption{}
    \label{fig:pe:204:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/output_00204_4_12_3.jpg}
    \subcaption{}
    \label{fig:pe:204:LBPs}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/output_00204_6_24_5.jpg}
    \subcaption{}
    \label{fig:pe:204:LBPb}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/00241.jpg}
    \subcaption{}
    \label{fig:pe:241:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/output_00241_4_12_3.jpg}
    \subcaption{}
    \label{fig:pe:241:LBPs}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/1/output_00241_6_24_5.jpg}
    \subcaption{}
    \label{fig:pe:241:LBPb}
  \end{subfigure}
  \caption{\subref{fig:pe:204:orig}, \subref{fig:pe:241:orig}  Original shoeprint impressions; \subref{fig:pe:204:LBPs}, \subref{fig:pe:241:LBPs} Output using LBP descriptors with radius 3 and 12 sample points; \subref{fig:pe:204:LBPb}, \subref{fig:pe:241:LBPb} Output using LBP descriptors with radius 5 and 24 sample points}
  \label{fig:pe:LPBexp}
\end{figure}

\par
To summarize, the modified learning strategy introduced for LBP and SIFT descriptor provides a robust feature pool, unlike the original one proposed by Guo et al. \cite{guo2012discriminative}.
Because the Fourier-Mellin feature pool was created using the original learning strategy, it calculates clear results  on the evaluation dataset and has lower performance on the noisy, training images.
However, in both cases it is more responsive on detailed areas with fine line structures, such as the bottom left side of Figure \ref{fig:pe:25} and bottom right side of Figure \ref{fig:pe:182}, than LBP and SIFT.
\par
There are, however, lower quality samples in the dataset available than the one presented on Figure  \ref{fig:pe:66}, where there is small contrast between the pattern and the background, see Figure  \ref{fig:int:cap:neg}, or there is a high amount of noise on the fore- and the background as well, see Figure \ref{fig:rw:lowFID}.
The feature pool is created based on a limited training set of seven images.
Incorporating more samples and altering the learning criteria, e.g. the given feature has to be found in a given ratio of training images and not on every one of them, leads to a broader feature pool.
Such a feature pool is possibly descriptive for a wider range of images, such as low quailty images, shoeprint images with fine-lined structures, low contrast images etc, as well.
However, this also increases the chance of selecting more noise descriptors into the final feature set.

\subsection{Fully-Automated Noise Elimination Algorithm}
\par
Figures \ref{fig:fans:denoise} and \ref{fig:fans:enhance} show the output of the proposed algorithm on every stage and present the final results as well.
The former one shows the output of the denoising step on three example shoeprint images from the FID-300 database.
In the second column the Wiener filtered images are shown, in the third one the Bilateral filtered images are displayed whereas the combination of those two pictures is in the last column.
Considering the background and the amount of noise in the images, a light smoothing is done during the denoising phase.
This decision was made to protect the valuable shoeprint information.
Since no previous knowledge is available about the properties of the shoeprint image, such as appearance and location of noise and the shoeprint, no aggressive filter kernel is used.
The  size of both kernels is small, being only 5x5, to make sure that the relevant shoeprint information is not distorted.
In the combined image, the contrast between fore- and background is increased.
Since both filters smooth the background area and preserve the shoeprint, their combination amplifies the blurring effect without damaging the shoeprint in the foreground. 
Despite the amplifying effect, structured noise elements, such as the white clutter on Figure \ref{fig:fans:235:orig}, are not eliminated.  
Chatterjee et al. \cite{chatterjee2011patch} proposed a method which calculates the estimated image for Wiener Filter based on geometrically and photometrically similar subregions of the input.
With such an estimation blurring, which eliminates the previously mentioned structured noise, is possible without damaging the foreground area.
Even though their approach had promising results, it was tested on data with Gaussian noise, similar to Figure \ref{fig:fans:241:orig}, and not with irregular cluttered noise seen in the majority of FID-300 images, for example in Figure \ref{fig:fans:235:orig} and in the top of Figure \ref{fig:fans:21:orig}.
\par
The second step of the algorithm is enhancement, where SMQT is applied.
The results are shown in Figures \ref{fig:fans:241:enhance}, \ref{fig:fans:235:enhance} and \ref{fig:fans:21:enhance}.
The benefits of the SMQT algorithm are ambiguous.
On the one hand, focusing on the middle area of Figures  \ref{fig:fans:241:enhance} and \ref{fig:fans:235:enhance} the difference between fore- and background is bigger than on the original image.
The contours of the shoeprint are more outstanding than on the original image.
Comparing Figure \ref{fig:fans:235:enhance} to Figure \ref{fig:fans:21:denoise}, the shoeprint is darker and there is a brighter area around the contours than on Figure \ref{fig:fans:21:denoise}.
On the other hand, not only the shoeprint but also the remaining noise elements became enhanced by SMQT.
Even though the denoising stage is finished, since only moderate smoothing was applied, there is a considerable amount of noise on the input images for SMQT.
The soft filtering is able to cope with Gaussian noise, such as on Figure \ref{fig:fans:241:denoise}, but it does not eliminate bigger clutter particles such as in the left side of Figure \ref{fig:fans:235:denoise} or in the upper region of Figure \ref{fig:fans:21:denoise}.
The SMQT enhancement makes these clutter prominent by increasing the color difference between the noise and the homogeneous background.
For example, the already mentioned patches on the left side of Figure \ref{fig:fans:235:denoise} are now more visible than on the original image \ref{fig:fans:235:orig}.
Additionally, SMQT has similar effects to Histogram Equalization: the background on Figure \ref{fig:fans:241:denoise} is near homogeneous, but after applying SMQT, its color changes are more obvious since the original pixel values are mapped now to a wider color range.
Despite its enhancing effect in the background, SMQT also increases the contrast between the outlines of the shoeprint and its surroundings which is a beneficial feature for the following, local binarization step.

\afterpage{
\begin{figure}[H]

\subfloat{
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241.jpg}
    \subcaption{}
    \label{fig:fans:241:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_wiener.jpg}
    \subcaption{}
    \label{fig:fans:241:wiener}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_bi5.jpg}
    \subcaption{}
    \label{fig:fans:241:bi}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_denoise.jpg}
    \subcaption{}
    \label{fig:fans:241:denoise}
  \end{subfigure}
  \caption{}
}

\subfloat{
    \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235.jpg}
    \subcaption{}
    \label{fig:fans:235:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_wiener.jpg}
    \subcaption{}
    \label{fig:fans:235:wiener}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_bi5.jpg}
    \subcaption{}
    \label{fig:fans:235:bi}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_denoise.jpg}
    \subcaption{}
    \label{fig:fans:235:denoise}
  \end{subfigure}
  \caption{}
}

\subfloat{
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
    \subcaption{}
    \label{fig:fans:21:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_wiener.jpg}
    \subcaption{}
    \label{fig:fans:21:wiener}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_bi5.jpg}
    \subcaption{}
    \label{fig:fans:21:bi}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_denoise.jpg}
    \subcaption{}
    \label{fig:fans:21:denoise}
  \end{subfigure}
  \caption{}
}

\caption{Example for the results of the denoising stage
				\subref{fig:fans:241:orig} Example image; \subref{fig:fans:241:wiener} Wiener filtered image; \subref{fig:fans:241:bi} Bilateral filtered image; \subref{fig:fans:241:denoise} Denoised image}
\label{fig:fans:denoise}

\end{figure}
}

\afterpage{
\begin{figure}[H]

\subfloat{
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241.jpg}
    \subcaption{}
	\label{fig:fans:241:o}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_enhnace.jpg}
    \subcaption{}
    \label{fig:fans:241:enhance}
  \end{subfigure}
 \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_threshold.jpg}
    \subcaption{}
    \label{fig:fans:241:bin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_result.jpg}
    \subcaption{}
    \label{fig:fans:241:out}
  \end{subfigure}
  \caption{}
}

\subfloat{
    \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235.jpg}
    \subcaption{}
	\label{fig:fans:235:o}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_enhnace.jpg}
    \subcaption{}
    \label{fig:fans:235:enhance}
  \end{subfigure}
 \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_threshold.jpg}
    \subcaption{}
    \label{fig:fans:235:bin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_result.jpg}
    \subcaption{}
    \label{fig:fans:235:out}
  \end{subfigure}
  \caption{}
}

\subfloat{
  \centering
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
    \subcaption{}
	\label{fig:fans:21:o}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_enhnace.jpg}
    \subcaption{}
    \label{fig:fans:21:enhance}
  \end{subfigure}
 \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_threshold.jpg}
    \subcaption{}
    \label{fig:fans:21:bin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_result.jpg}
    \subcaption{}
    \label{fig:fans:21:out}
  \end{subfigure}
  \caption{}
}

\caption{Example for the results of the enhancement and postprocessing stage
				\subref{fig:fans:241:o} Example image; \subref{fig:fans:241:enhance} Enhanced image; \subref{fig:fans:241:bin} Binarized image ; \subref{fig:fans:241:out} Output image}
\label{fig:fans:enhance}

\end{figure}
}
\par
The closing step of the algorithm is postprocessing, consisting of binarization (the third column of Figure \ref{fig:fans:enhance}) and line-clutter elimination (in the fourth column of Figure \ref{fig:fans:enhance}).
As expected NBM, being a local edge sensitive thresholding algorithm, takes advantage of the contrast difference between the outlines of the shoeprint  and its neighborhood and finds the silhouettes. 
On all three images, see Figure \ref{fig:fans:241:bin}, Figure \ref{fig:fans:235:bin} and Figure \ref{fig:fans:21:out}, the contours of the patterns are visible.
However, the drawbacks of NBM discussed in the previous chapter and the enhanced noise by SMQT are evident, since among the correct outlines noise is generated.
In the background and between the space of the outlines of the shoeprint additional small structures are visible.
For this reason, the closing step of small and open structure elimination is executed.
In Figures \ref{fig:fans:241:out} and \ref{fig:fans:235:out} the majority of background is cleared up and the shoeprint pattern is shown clearly.
On the other hand, in Figure \ref{fig:fans:21:out} the original pattern is not recognizable anymore, as along with the background noise, relevant contour edges were also eliminated, and noise edges attached to the shoeprint were kept.
\par
The presented results indicate two things.
First, NBM with the current postprocessing technique works  on shoeprint images with fine lines, see Figure \ref{fig:fans:241:orig}, and with detailed structures, see Figure \ref{fig:fans:235:orig} than on shoeprint images consisting blob-like elements, such as in Figure \ref{fig:fans:21:orig}.
As already discussed and shown, NBM tends to generate line noise on homogenous areas.
If the shoeprint consists of dense pattern, there is no big, homogenous background between the different structure elements, thus no noise is generated.
To solve this problem, Saxena et al. \cite{saxena2019niblack} analyze several improvements for NBM to make it more robust in such conditions and in low quality data.
Second, the heuristics which the two postprocessing steps are based on do not apply to every shoeprint image of the database.
In Figure  \ref{fig:fans:21:bin}, despite the noise, the shoeprint is clearly recognizable, whereas in Figure \ref{fig:fans:21:out} no shoeprint is seen anymore.
This implies that the settings of the structure elimination algorithms are not detailed enough, and that more sophisticated postprocessing is needed, which covers further shoeprint images of the database.
Shoeprint edges were eliminated because they were shorter than the defined threshold.
The threshold for edge length were not changed during testing, thus a possible solution for the problem is to always adjust the parameter according to the current image.
This adjustment is however difficult, since there is no a-priori knowledge about the given shoeprint image available.
Alternatively, looking at Figure  \ref{fig:fans:21:out}, the outlines of the shoeprint were found originally, but because of the noise they were not connected.
In the application only 8-neighborhood is considered, a more allowing connectivity criteria, such as neighborhood in a given range, leads to closed contour edges.
But this causes more connectivity among noise as well.
Furthermore, it also magnifies the second problem occurring in Figure \ref{fig:fans:21:out}, that noise edges are connected to the contours, thus are not eliminated.
In conclusion, an additional postprocessing step is needed to examine the remaining edges and to determine which connection was made unintentionally.
\par
Based on the experiments presented in this section two factors influence the output of the algorithm.
These are image quality, such as the appearance of noise and contrast between fore- and background, and the properties of the shoeprint, e.g. the density and size of the unit pattern structure.
In conclusion, this algorithm is specialized in a given type of shoeprint in its current form, namely the ones with dense patterns or fine-lined structures. 
In such cases, despite cluttered noise, Figure \ref{fig:fans:235:o}, promising results were generated.
To expand the usability of this algorithm for other shoeprint images of FID-300 further improvements, such as detailed noise model for Wiener Filter and variable settings for the postprocessing heuristics, are needed. 


%\afterpage{
\begin{figure}[H]

\subfloat{
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/orig.jpg}
    \subcaption{}
	\label{fig:sans:21}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/20_mean.jpg}
    \subcaption{}
    \label{fig:sans:21:20_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/20_cent.jpg}
    \subcaption{}
    \label{fig:sans:21:20_cent}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/25_mean.jpg}
    \subcaption{}
    \label{fig:sans:21:25_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/25_cent.jpg}
    \subcaption{}
    \label{fig:sans:21:25_cent}
  \end{subfigure}
  \caption{}
}

\subfloat{
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/orig.jpg}
    \subcaption{}
	\label{fig:sans:25}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/20_mean.jpg}
    \subcaption{}
    \label{fig:sans:25:20_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/20_cent.jpg}
    \subcaption{}
    \label{fig:sans:25:20_cent}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/25_mean.jpg}
    \subcaption{}
    \label{fig:sans:25:25_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/25_cent.jpg}
    \subcaption{}
    \label{fig:sans:25:25_cent}
  \end{subfigure}
  \caption{}
}

\caption{Example results of the Shoeprint Enhancement algorithm
				\subref{fig:sans:21} Example image; \subref{fig:sans:21:20_mean} Enhanced image using average noise model and 20 clusters for gradient classification; \subref{fig:sans:21:20_cent} Enhanced image using centroids noise model and 20 clusters for gradient classification; \subref{fig:sans:21:25_mean} Enhanced image using average noise model and 25 clusters for gradient classification; \subref{fig:sans:21:25_cent} Enhanced image using centroid noise model and 25 clusters for gradient classification}
\label{fig:sans:res1}

\end{figure}
%}

\subsection{Shoeprint Enhancement with Non-Local Means}
\par
In this section the experimental results of the Shoeprint Enhancement Algorithm with Non-Local Means are presented and the performance is discussed.
In Figures \ref{fig:sans:res1} and \ref{fig:sans:res2} example results are shown, the original samples are in the first column, the results using 20 classes for gradient clustering are shown in the second and third column, the output using 25 classes are in the fourth and fifth column.
Column two and four illustrate the results when the average noise model was used for pattern enhancement, whereas column three and five represent the output in case of using the centroids of the k-means algorithm.
\par
The example images, see  \ref{fig:sans:res1} and \ref{fig:sans:res2}, show that varying the algorithm parameters makes a significant difference when the input had low quality, namely in shoeprint images with low shoeprint-to-noise ration.
This is explained with two factors.
First, high quality images have less noise on the shoeprint, see Figures \ref{fig:sans:21} and \ref{fig:sans:204}, or the noise is less varying, see Figure \ref{fig:sans:21}, than on a lower quality sample, as in Figure \ref{fig:sans:174}.
Thus, using different noise models do not make a difference, since little noise needs to be removed.
Second, using 20 and 25 classes for the non-local means algorithm is overstated.
In high quality images, there is no high variance between the properties of edges, thus classes with little amount of members are generated.
However, choosing high amount of classes was an intentional decision.
First, the structure of the shoeprint image is preserved, when the means are calculated on a high, 20 and 25, amount of subgroups.
Second, using fewer, less than 20, classes leads to eliminating valuable shoeprint information when deleting the members of the most populated class.
Comparing the output of the two different kinds of noise models, there is little visible difference between their performance.
Both models were able to preserve the pattern information where there is no, see  \ref{fig:sans:25}, or little noise, see  \ref{fig:sans:21}, on the shoeprint area.
Focusing on small details, such as the top of Figure \ref{fig:sans:174}, the average noise model is more aggressive than the centroid one.
That observation corresponds to the calculation of the noise representation.
Taking the average of the descriptors of every noise pixel results in a less detailed model than calculating more representatives, thus it is more destructive than the centroid model.


\afterpage{
\begin{figure}[H]

\subfloat{
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/orig.jpg}
    \subcaption{}
	\label{fig:sans:174}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/20_mean.jpg}
    \subcaption{}
    \label{fig:sans:174:20_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/20_cent.jpg}
    \subcaption{}
    \label{fig:sans:174:20_cent}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/25_mean.jpg}
    \subcaption{}
    \label{fig:sans:174:25_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/25_cent.jpg}
    \subcaption{}
    \label{fig:sans:174:25_cent}
  \end{subfigure}
  \caption{}
}


\subfloat{
  \centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/orig.jpg}
    \subcaption{}
	\label{fig:sans:204}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/20_mean.jpg}
    \subcaption{}
    \label{fig:sans:204:20_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/20_cent.jpg}
    \subcaption{}
    \label{fig:sans:204:20_cent}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/25_mean.jpg}
    \subcaption{}
    \label{fig:sans:204:25_mean}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/25_cent.jpg}
    \subcaption{}
    \label{fig:sans:204:25_cent}
  \end{subfigure}
  \caption{}
}

\caption{Example results of the Shoeprint Enhancement algorithm
				\subref{fig:sans:174} Example image; \subref{fig:sans:174:20_mean} Enhanced image using average noise model and 20 clusters for gradient classification; \subref{fig:sans:174:20_cent} Enhanced image using centroids noise model and 20 clusters for gradient classification; \subref{fig:sans:174:25_mean} Enhanced image using average noise model and 25 clusters for gradient classification; \subref{fig:sans:174:25_cent} Enhanced image using centroid noise model and 25 clusters for gradient classification}
\label{fig:sans:res2}

\end{figure}
}

\par
In the images of Figure \ref{fig:sans:204} significant shoeprint parts are missing on the bottom area.
That is the combined effect of those steps where the heuristic is assumed, that small regions represent noise elements rather than relevant shoeprint information.
Therefore the fine-lines of the shoeprint pattern are wrongly considered as noise, and they are eliminated.
\par
In Figure \ref{fig:sans:pip} the entire pipeline of the Shoeprint Enhancement algorithm is shown in three example images.
In Figures  \ref{fig:sans:pip:21:bg}, \ref{fig:sans:pip:204:bg} and \ref{fig:sans:pip:239:bg} the binary mask is shown, where white denotes the background and black the shoeprint area.
Inspecting the images, the majority of the background is labeled correctly, however, two limitations are visible.
The first one is the bottom area of Figure \ref{fig:sans:pip:204:bg}, where half of the fine-lined area of the actual shoeprint is cropped.
The background detection is not able to distinguish between the structured noise in the background and the detailed shoeprint in the foreground.
Furthermore, the fine-lines cover a small area of the three different sized neighborhoods when calculating the correlation with the noise representation, therefore high similarity is noted.
In this context the fine-lines of the shoeprint pattern are considered as "noise" when comparing them to the noise representation according to the user input.
Because of the robustness of the Fourier-Mellin features, the thin lines of the shoeprint impression have little influence, thus that area is falsely labeled.
The other one is in Figures \ref{fig:sans:pip:21:bg}, where, because of the significant change within the background, parts of the top area are falsely labeled as foreground.

\par
The appearance of the input image after eliminating the noise in the foreground is shown in the third column.
Figures  \ref{fig:sans:pip:204:bg} and \ref{fig:sans:pip:239:bg} illustrate that the noise suppression technique does not damage the pattern area, the difference between the fore- and the background is preserved.
In Figure \ref{fig:sans:pip:21} there is significant noise on the top region, which is successfully suppressed in Figure \ref{fig:sans:pip:21:enh}.
This indicates that, as expected, the noise and the shoeprint have different Fourier representations.
Working in the Fourier domain, the noise frequencies are eliminated and the shoeprint is successfully reconstructed.
\par
In the fourth column, the results after applying the non-local means algorithm are shown.
Focusing to the background, nearly homogeneous background is created without applying the noise mask on the images.
However, in Figure \ref{fig:sans:pip:204:grad} a weakness of the algorithm is also visible.
The non-local means calculation eliminates the majority of pattern information in both top and bottom area of the original shoeprint.
Combined with structured background noise, several mixed classes are generated where noise and shoeprint data occur simultaneously.
The artifacts of this phenomenon are visible not only on the said shoeprint area, but also in the middle of the image, where noise structures were enhanced appearing as bright edges similar to the remaining shoeprint edges.
\par
Figures \ref{fig:sans:pip:21:th}, \ref{fig:sans:pip:204:th} and \ref{fig:sans:pip:239:th} show the binarized images after applying the background mask.
Because of the nearly homogeneous background achieved by non-local means, the thresholding method is able to preserve the extracted lines.
With masking, the falsely clustered edges from the previous step, such as the ones in the middle of Figure \ref{fig:sans:pip:204:grad}, are deleted.
\par
In the last column the final results are shown after the postprocessing step of eliminating small regions.
Comparing Figure \ref{fig:sans:pip:21:th} to  Figure \ref{fig:sans:pip:21:res} the latter is less cluttered, there is less noise in the background, small lines around the contours of the shoeprint are eliminated successfully while no relevant edges, the outlines of the shoeprint, were deleted.
However, in Figure \ref{fig:sans:pip:204:res} the shoeprint information is further damaged by the postprocessing step.
Observing the right side of the bottom area, it is visible that several shoeprint contours were eliminated.
The postprocessing step assumes that the shoeprint consists of bigger structures such as filled geometries on Figure \ref{fig:sans:pip:21} or broad edges on Figure \ref{fig:sans:pip:239}, thus the fine-lined edges of Figure \ref{fig:sans:pip:204} are automatically considered as noise instead of shoeprint.


\afterpage{
\begin{figure}[H]

\subfloat{
  \centering
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/orig.jpg}
    \subcaption{}
	\label{fig:sans:pip:21}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/mask.jpg}
    \subcaption{}
    \label{fig:sans:pip:21:bg}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/enhance.jpg}
    \subcaption{}
    \label{fig:sans:pip:21:enh}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/grad.jpg}
    \subcaption{}
    \label{fig:sans:pip:21:grad}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/th.jpg}
    \subcaption{}
    \label{fig:sans:pip:21:th}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/res.jpg}
    \subcaption{}
    \label{fig:sans:pip:21:res}
  \end{subfigure}
  \caption{}
}

\subfloat{
  \centering
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/orig.jpg}
    \subcaption{}
	\label{fig:sans:pip:204}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/mask.jpg}
    \subcaption{}
    \label{fig:sans:pip:204:bg}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/enhance.jpg}
    \subcaption{}
    \label{fig:sans:pip:204:enh}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/grad.jpg}
    \subcaption{}
    \label{fig:sans:pip:204:grad}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/th.jpg}
    \subcaption{}
    \label{fig:sans:pip:204:th}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/res.jpg}
    \subcaption{}
    \label{fig:sans:pip:204:res}
  \end{subfigure}
  \caption{}
}

\subfloat{
  \centering
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/orig.jpg}
    \subcaption{}
	\label{fig:sans:pip:239}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/mask.jpg}
    \subcaption{}
    \label{fig:sans:pip:239:bg}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/enhance.jpg}
    \subcaption{}
    \label{fig:sans:pip:239:enh}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/grad.jpg}
    \subcaption{}
    \label{fig:sans:pip:239:grad}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/th.jpg}
    \subcaption{}
    \label{fig:sans:pip:239:th}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/res.jpg}
    \subcaption{}
    \label{fig:sans:pip:239:res}
  \end{subfigure}
\caption{}
}

\caption{The whole pipeline of the Shoeprint Enhancement algorithm using 25 classes and centroids for noise elimination on pattern
				\subref{fig:sans:pip:21} Example image; \subref{fig:sans:pip:239:bg} Background mask; \subref{fig:sans:pip:21:enh} Enhanced image after eliminating noise on the foreground; \subref{fig:sans:pip:21:grad} Gradient image after calculating non-local means;  \subref{fig:sans:pip:21:th} Binarized image using adaptive thresholding; \subref{fig:sans:pip:21:res} Resulting image after eliminating small structures}
\label{fig:sans:pip}

\end{figure}
}

\par
Along misinterpreting fine-line structures of the shoeprint, there is an additional limitation of the proposed method.
When there is low contrast between the background and foreground, or the foreground is highly cluttered, no robust estimation can be made about the background area and about the dominant appearance of the noise. 
Such a case is shown in Figure \ref{fig:sans:noise}.
In this case the output is computed solely based on the gradient-based non-local means method and the postprocessing step.
When the determined background area is too small or too big, no masking is made after binarizing the image and no noise is eliminated from the foreground, since no noise model is calculated.
In such cases several enhancement steps are not exploited, thus the quality of the output is lower. 

\afterpage{
\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.3\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noise/orig.jpg}
    \subcaption{}
    \label{fig:sans:noise:orig}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noise/mask.jpg}
    \subcaption{}
    \label{fig:sans:noise:mask}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\columnwidth}
    \centering
   \includegraphics[width=\textwidth]{5/noise/res.jpg}
    \subcaption{}
    \label{fig:sans:noise:res}
  \end{subfigure}
 
  \caption{Output of the Shoeprint Enhancement algorithm, where no estimation about the background can be made; \subref{fig:sans:noise:orig} Example image from the dataset; \subref{fig:sans:noise:mask} Calculated background mask; \subref{fig:sans:noise:res} Output of the Shoeprint Enhancement algorithm}
  \label{fig:sans:noise}
\end{figure}
}

\par
The experimental results show that the Shoeprint Enhancement algorithm provides more precise results, see Figure \ref{fig:sans:25:25_cent}, than the firstly introduced Feature Learning approach, see Figure \ref{fig:pe:25:FM}.
Furthermore, as in Figure \ref{fig:sans:pip:21:res}, it copes better with noise than the fully-automated method, see Figure \ref{fig:fans:21:out}.
On the one hand, the Noise Elimination algorithm achieves better results on detailed, fine-lined shoeprints, as seen on Figures \ref{fig:fans:235:out} and \ref{fig:fans:241:out} then the Shoeprint Enhancement one, see Figure \ref{fig:sans:pip:204:res}.
On the other hand, the Shoeprint Enhancement algorithm has overall higher performance than the two previous ones.
First, it is able to deal with low-quality samples, see Figure \ref{fig:sans:174:25_cent}, as well.
Second, its effectiveness is not limited to one kind of shoeprint outsule pattern such as the previous approach, which only provided good results on densely structured shoeprints, for example Figures \ref{fig:fans:235:out}, \ref{fig:fans:241:out}. 
Based on the evaluations in the last three sections, the last approach proved to be the most powerful technique from the three proposed algorithms.
To get a detailed overview about its performance, additional quantitative tests are conducted in the following section, and the results are further discussed.

\section{Quantitative Evaluation}
\par
The visual improvements resulting from the three different algorithms were discussed in the previous sections.
According to the qualitative evaluation, the Shoeprint Enhancement Algorithm with Non-Local Means had the most promising results, since it provided more accurate shoeprint layout than the Feature Learning algorithm and is less shoeprint specific than the Noise Elimination pipeline.
Therefore further tests are conducted.
In this section the quantitative results on a selection of images from FID-300 are presented and discussed.
First, the testing dataset is described, afterwards, following the presentation of the evaluation method, the quantitative results on the dataset are discussed.

\subsection{Testing Dataset}

The testing database consist of 20 images.
The selected images and the output of the algorithm are shown in the Appendix \ref{AppA}.
Low quality and heavily cluttered images are also part of the experimental database.
Since the evaluation in the previous chapter shows that the Shoeprint Enhancement approach is not effective on samples with detailed, fine line structures, those images \ref{appA:204}, \ref{appA:223}, \ref{appA:241} are excluded from the evaluation.
Furthermore, it was mentioned as well that in case of low intensity difference no background estimation is possible. 
Therefore images \ref{appA:171}, \ref{appA:217} and \ref{appA:232} are also disregarded during the evaluation.
Additionally, to save computational time, the corresponding reference images of every member of the minimal database were also selected.

\subsection{Evaluation Method}
\par
For evaluation the matching accuracy of the original and the enhanced images is compared based on three basic descriptors.
If the basic descriptors, without embedding  a more complex pipeline, achieve higher accuracy on the processed  images than on the original ones, it implies successful enhancement. 
The experimental evaluations conducted in the field indicate \cite{rida2019forensic} that a complex approach increases the matching accuracy.
However, when one given pipeline is used for evaluation, it is not clear, if the enhancement is optimized for that specific pipeline or whether it is generally valid.
Evaluating based on the feature descriptors which are the core parts of image matching algorithms provides general results on the performance.
\par
The descriptors used for matching are Fourier-Mellin features, SIFT and SURF, which are popular techniques for shoeprint, fingerprint and for natural image description.
Fourier-Mellin transform was proposed for shoeprint description several times \cite{gueham2008automatic}, \cite{richetelli2017classification}, \cite{wu2019crime}.
Along shoeprint description \cite{nibouche2009rotation}, \cite{richetelli2017classification} SIFT is also favored for fingerprint \cite{zhou2011adaptive} and for tattoo identification \cite{yi2015impact}, \cite{han2013tattoo}.
Although SURF is not common for shoeprint recognition, it is popular in related areas, such as fingerprint description \cite{jahan2017robust}, tattoo matching \cite{yi2015impact} and natural texture description \cite{prabhakar2012lbp}.
The descriptors are extracted from every 4\textsuperscript{th} pixel in both x and y direction.
\par
For matching the extracted features, two algorithms are used: Brute-Force matching (BF) \cite{schaeffer1993re} and Fast Library for Approximate Nearest Neighbors (FLANN) \cite{muja2009fast}.
BF compares a feature of one image to every other features of the other one, and sets the closest one as match, whereas FLANN picks the best matches from the neighborhood, it does not examine all possible feature points, so it is faster than BF.
In both cases Euclidean distance is used to compare the descriptors.
To filter the provided matches Lowe's ratio test is used \cite{lowe2004distinctive}.
With this test the false positive matches are eliminated.
To do so, the distance ratio of the two nearest matches to a given feature is examined, and it is accepted if it is below a given threshold.
The criteria for 'good match' is set to 0.8 for testing, as Lowe  \cite{lowe2004distinctive} suggests.
It is examined how many 'good matches' are there between the shoeprint- and the reference images separately.
During the matching process, the shoeprint image is compared to every reference image.
At the end, the reference images are ordered according to the amount of 'good matches' to the same shoeprint image.
The one with the most 'good matches' is the first, the one with the second most 'good matches' is the second and so on.
The rank of the Ground Truth reference image of the given shoeprint image is noted, and the final score is made by averaging the noted ranking values  of every image from the testing dataset.
That means if the Ground Truth image of every member of the dataset had the most 'good matches', thus the best ranking, the final score is 1.
Lower performance is indicated with higher score, correct matching results with lower score.

\subsection{Evaluation}
\par
The quantitative results of the Shoeprint Enhancement Algorithm with Non-Local Means method are evaluated based on the performance of  the three basic feature descriptors.
To create a baseline the original shoeprint images were matched to the reference images.
For reproducibility the scores of all four possible settings  of the enhancing algorithms and of the baseline are given in the Appendix \ref{AppB}.
Table \ref{tab:ov} gives an overview of the accumulated performance of the discussed settings.
The parameter settings tested are 20 classes for the non-local means clustering using average noise model (20A) or using the centroid noise model (20C) and 25 classes for the non-local means clustering using average noise model (25A) or using centorid noise model (25C).
In the upper side of the cells the average score of both matching techniques, BF and FLANN, are shown separately.
In the lower part of the cells the average score of both matching techniques is displayed.
In the last column the average score of all descriptors, F-M, SIFT and SURF, using both matching techniques is shown.
In the table, multiple cells are highlighted, the one in the first row shows the best score in the baseline, 4.43, provided by FLANN matching the SIFT descriptors.
The other gray cells show the scenarios where the enhanced images did not achieve the best performance of the baseline.
The cells with blue background highlight the scenarios, when the enhanced images achieved and outperformed the best baseline result, darker color indicates better accuracy.
Bright blue is used for scores above 4, middle blue indicates results between 4 and 3.50, and better performance than that is highlighted with dark blue.
The accumulated values, the average of the results, is not highlighted.
To understand the performance of the proposed enhancing method, this table is analyzed in the followings.
\par
Focusing on the first (F-M)  and thrid (SURF) column of the table, the matching of the enhanced shoeprints always outperforms the corresponding baseline regardless the pipeline settings.
Furthermore, the Fourier-Mellin descriptor achieved better results than the best baseline  except for the one case when 20A is used. 
However, in the second (SIFT) and third (SURF) column of the table this is not the case.
Even though SURF always outperformed the corresponding baseline, which is 5.64, in case of 25C and 20C the accuracy of the overall best baseline performance of 4.43 is not met.
In case of SIFT, the results of the enhanced images are significantly lower than of the original images.
Using BF matching, the enhanced images outperform the corresponding baseline, but achieve only two times out of four better accuracy than the overall best baseline result.
In case of FLANN matching, the enhanced images always have lower accuracy than the corresponding baseline, which is the best performance noted for the original images. 
Since the enhnaced images are binarized, the usage of SIFT and SURF is not ideal.
SIFT is a gradient based descriptor and the majority of that information is lost when thresholding an image.
Furthermore, local invariant descriptors, such as SIFT and SURF, usually have lower performance on binary images than on grayscale samples \cite{wang2017manifold}.
However, SURF is generally more robust than SIFT which also explains its better performance even using on binarized images \cite{bay2006surf}.

\definecolor{custGrey}{rgb}{0.5,0.5,0.5}
\definecolor{cDarkBlue}{rgb}{0, 0.55, 1}
\definecolor{cMidBlue}{rgb}{0.5, 0.8, 1}
\definecolor{cBrightBlue}{rgb}{0.86, 0.94, 1}
\newcolumntype{?}{!{\vrule width 1.2pt}}

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c?cc|cc|cc?c}
& \multicolumn{2}{c|}{ F-M } & \multicolumn{2}{c|}{ SIFT } & \multicolumn{2}{c?}{ SURF } & \multirow{2}{*}{ Average Results } \\
& BF & FLANN & BF & FLANN & BF & FLANN & \\
\specialrule{2.5pt}{1pt}{1pt}
Baseline & 5.86 &  5.86 & 5.57 & \cellcolor{custGrey} 4.43 & 6.50 & 5.64 & \multirow{2}{*}{ 5.64 } \\
& \multicolumn{2}{c|}{ 5.86 } & \multicolumn{2}{c|}{ 5.00 } & \multicolumn{2}{c?}{ 6.07 } & \\
\specialrule{1.1pt}{1pt}{1pt}
25 Average  &  \cellcolor{cMidBlue} 3.86 &  \cellcolor{cMidBlue} 3.79 &  \cellcolor{cBrightBlue} 4.21 & \cellcolor{custGrey}5.57 &  \cellcolor{cMidBlue} 3.93 &  \cellcolor{cBrightBlue} 4.43 & \multirow{2}{*}{ 4.30 } \\
(25A)& \multicolumn{2}{c|}{ 3.82 } & \multicolumn{2}{c|}{ 4.89 } & \multicolumn{2}{c?}{ 4.18 } & \\
\hline
25 Centroids  & \cellcolor{cDarkBlue} 3.29 & \cellcolor{cDarkBlue} 3.21 &  \cellcolor{cBrightBlue} 4.29 &  \cellcolor{custGrey} 5.14 &   \cellcolor{cBrightBlue} 4.43 & \cellcolor{custGrey} 5.36 & \multirow{2}{*}{ 4.29 } \\
 (25C) & \multicolumn{2}{c|}{ 3.25 } & \multicolumn{2}{c|}{ 4.71 } & \multicolumn{2}{c?}{ 4.89 } & \\
\hline
20 Average & \cellcolor{cBrightBlue} 4.43 &  \cellcolor{custGrey} 4.57 &  \cellcolor{custGrey} 4.93 &  \cellcolor{custGrey} 5.21 & \cellcolor{cDarkBlue} 3.00 & \cellcolor{cMidBlue} 3.64 & \multirow{2}{*}{ 4.30 } \\
 (20A) & \multicolumn{2}{c|}{ 4.50 } & \multicolumn{2}{c|}{ 5.07 } & \multicolumn{2}{c?}{ 3.32 } & \\
\hline
20 Centorids & \cellcolor{cDarkBlue} 3.29 & \cellcolor{cDarkBlue} 3.07 &  \cellcolor{cBrightBlue} 4.43 & \cellcolor{custGrey}5.71 & \cellcolor{cMidBlue} 3.71 &  \cellcolor{custGrey} 4.50 & \multirow{2}{*}{ 4.12 } \\
(20C) & \multicolumn{2}{c|}{ 3.18} & \multicolumn{2}{c|}{ 5.07 } & \multicolumn{2}{c?}{ 4.11 } & \\
\end{tabular}
\captionof{table}{Overview about the performance of the analyzed pipelines}
 \label{tab:ov} 
%\end{sidewaystable}
\end{minipage}

\par
The Fourier-Mellin features (first column) always outperformed their corresponding baseline, and failed only once to meet or outperform the best accuracy achieved by the original images.
The two lowest results of the Fourier-Mellin descriptors were provided using 20A settings.
Furthermore, the 25A settings only outperformed 20A, but no other testing scenario.
The average noise model caused, thus, lower performance.
The average noise model is more destructive than the centroid model.
Using the centroid representation, the used noise model is adjustable to the given part of the shoeprint image.
In this way the chance is lower that shoeprint information is also eliminated. 
Using the centroid model causes at least 0.5 points better score against the average model by comparing the best, 3.79, and the worst, 3.29, accuracy values achieved by average and centroid noise models respectively.
\par
Considering the overall accuracy (last column) of the different settings, the enhanced matching always provides better results than the accumulated baseline.
Furthermore, the overall accuracy is always higher than the best result in the baseline.
Comparing the variable settings, there is small difference between the best and the worst scores.
20C provided the highest performance achieving 4.12, whereas the other scenarios achieved 4.30 and 4.29 accuracy only.
That means, that using less classes for the non-local means algorithm and calculating a detailed noise model leads to 0.18 points better accuracy.
The detailed noise model led to slightly better by 0.01 results, even though higher, 25 instead of 5, amount of classes were defined.
In case of 25C the performance of 4.29 is noted, whereas the other scenarios using the average model achieved 4.30.
Clustering the edges into less classes enforces the elimination of more edges.
Since the cluster with the highest cardinality is assumed to be a noise class,  owning the fact that using less classes the clusters have higher cardinality  in average than using more classes, the chance is higher that more noise-edges are eliminated.
The number of classes is, however, a crucial parameter.
When lowering this value more mixed clusters are created where noise and shoeprint edges are placed in the same cluster.
In that case, when the assumed noise cluster is eliminated, valuable parts of the shoeprint are also deleted.
Moreover, using small number of classes further details of the input are also eliminated when applying the non-local means algorithm.
The good performance using 20 classes indicates that the amount of classes is sufficient, and do not cause the elimination of potential shoeprint information.
\par
The experimental results presented in this chapter show that the proposed image enhancement algorithm supports accuracy improvement.
The performance of the Shoeprint Enhancement  algorithm was tested on a versatile subset of images selected from the FID-300 dataset.
However, the proposed method needs further improvement to be able to handle all possible shoeprint images of the FID-300 dataset.
For known limitations of the algorithm discussed in the previous section, shoeprint images with detailed, fine line structures and low contrast between fore- and background were excluded from testing.
The robustness and the high-quality of Fourier-Mellin features is promising, since many current shoeprint matching algorithms already use them for feature description  \cite{gueham2008automatic}, \cite{richetelli2017classification}, \cite{wu2019crime} or are based on other frequency domain descriptors \cite{algarni2008novel}, \cite{wang2014automatic}, \cite{katireddy2017novel}. 
\section{Summary}
\par
In this chapter the evaluation of the algorithms were presented.
It was discussed, if the methods are applicable for shoeprint images of the FID-300 dataset and the limitations were given.
The qualitative evaluation shows that all three methods delivered promising results in given scenarios.
The Feature Learning algorithm distinguish coarsely between the shoeprint and the background of the input, the exact outlines of the shoeprint are not found.
With the extension of the training dataset, a more accurate labeling is possibly reachable.
The Noise Elimination method provides promising results in case of specific shoe outsole patterns, such as the ones with detailed pattern and the ones containing fine-line structures.
Based on the evaluations, the Shoeprint Enhancement Algorithm with Non-Local Means is the most promising one, since unlike the Noise Elimination Algorithm it is not limited to one kind of shoeprint, and compared to the Feature Learning Algorithm it delivers more exact and detailed output, see Figures \ref{fig:sans:res1} and \ref{fig:pe:25}.
\par
Combining all three approaches the discussed advantage of one technique can suppress the limitation of the other two.
Even though the Shoeprint Enhancement Algorithm with Non-Local Means outperforms the other two methods, being semi-automated and not being able to cope with fine line structures, it has limitations. 
But letting the Feature Learning algorithm to determine the approximate outline of the shoeprint, and calculating the exact background with the currently used method in the Shoeprint Enhancement Algorithm replaces the now necessary user input at the beginning of the algorithm.
Additionally, the Noise Elimination method only provided reasonable output, if the input is not noisy and detailed line structure.
That is, however, the case that the Shoeprint Enhancement pipeline is not able to handle. 
In case of fine-lined shoeprints, processing the shoeprint image with the Shoeprint Enhancement method, and after applying the background mask finishing the calculation with the Noise Elimination approach, potentially solves the above issue.
The preprocessing done with the Shoeprint Enhancement method increases the quality that the Noise Elimination approach needs, and the latter one is able to identify the detailed pattern which the Shoeprint Enhancement technique eliminates immediately.
In this way the functionality of the proposed algorithms is extended and their current limitations are compensated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
%\par
%Forensic shoeprint image retrieval and identification is an extensively researched topic in the last 20 years.
%A great amount of publications is available and many methods were introduced for best shoeprint pattern recognition.
%However, no structured knowledge is available on this field, thus results are uncertain, the solved and open problems are unclear and the publications are incoherent.
%Because of the absence of universal, high-quality dataset the algorithms are not comparable, the limitations and strengths of the approaches are unclear and it is ambiguous if they are applicable in real-life use-cases as well.


\par
Forensic shoeprint identification is an extensively studied research area \cite{rida2019forensic}, nevertheless there are several possibilities for improvement.
In this thesis, three different approaches for forensic shoeprint enhancement are introduced, to increase the contrast between the shoeprint and its background, and to suppress the noise in the shoeprint images.
Based on the available shoeprint recognition literature and research done in related fields such as fingerprint and natural image enhancement, three prototypical mathods are presented and compared to increase the quality of shoeprint images from the FID-300 dataset.
\par
The first one is based on the idea of feature learning.
Samples from the FID-300 dataset were annotated pixel-wise, to accurately distinguish between the fore- and the background area.
During the training process, the features of the same shoe outsole pattern on different shoeprint images are extracted and compared.
First, descriptors are extracted from the fore- and the background separately, and those foreground descriptors which are similar to at least one background feature are eliminated. 
Hence, by noise distorted descriptors are excluded.
Afterwards, the remaining descriptors are compared and added to the final selection, if they occur in every shoeprint image of the same shoe outsole pattern.
In this way the robust descriptors are selected.
When processing a shoeprint image, the descriptors are extracted from every pixel and their similarity to the learnt descriptors is examined.
High similarity indicates shoeprint information in that area, low similarity stands for noise and background.
\par
The second proposed algorithm enhances the shoeprint in three steps.
First, the shoeprint image is smoothed using Wiener and Bilateral filter.
Using edge and information sensitive filters with small kernel settings, it is secured that the outlines of the shoeprint are preserved.
Afterwards, the contrast between the shoeprint and its background is increased using the Successive Mean Quantization Transfrom.
This step also prepares the shoeprint image for binarization.
The image is thresholed using the local Niblack Binarization Method.
To eliminate the additional edges generated by residual noise and thresholding artifacts, heuristics are applied which delete small, disconnected edges in the image.
\par
Lastly, a shoeprint enhancement method based on the non-local means algorithm is presented.
At the beginning, a  noise descriptors are extracted based on the multiple-sized neighborhood specified by user input.
Based on the descriptors, the background area of the shoeprint image is determined and a noise model is built.
The noise on the foreground, where the shoeprint is assumed to be located, is suppressed by subtracting the Fourier representation of the noise model from the Fourier representation of the foreground in the frequency domain.
The outlines of the shoeprint are then enhanced, contrast between them and their surroundings is increased, by applying the non-local means method.
The classification criteria is the gradient information of the pixels.
Gradients provide information about the edge properties of a given area, thus, assuming that the shoeprint pattern and the random noise on the image have different edge features, the elements of the shoeprint are processed separately from the noise.
Finally, the binary image is calculated using adaptive thresholding.
\par
Since there is limited research in the area of forensic shoeprint enhancement available, the presented approaches show three possibilities to develop a methodology for the enhancement of shoeprint images.
The qualitative evaluation of the algorithms shows that all three methods provide quality improvement, namely, suppression of noise, and clearing or extracting the outlines of the shoeprint. 
Based on the qualitative evaluation, the enhancement with non-local means is the most promising, since it provides more accurate outlines, and it is not limited to given properties of the shoeprint with dense pattern and fine-line structures.
The quantitative evaluation performed with descriptors already used in forensic image identification shows that the enhanced images achieve better matching results.
With further development and potential combination of the proposed methods, a powerful and versatile preprocessing pipeline can be built which is able to enhance shoeprint impressions, thus increase the matching accuracy.

\section*{Future Work}

\par
There are several approaches in research available which were not discussed in this work yet.
Chen et al. \cite{chen2013hierarchical} introduced a hierarchical voting score for finger- and palmprint identification.
Wu et al. \cite{wu2019losgsr} also proposed a voting technique for real-life forensic images incorporating the opinion of human experts.
They defined a manifold ranking method based on the shoeprint identification of forensic specialists.
Along the judgments of the experts, the neighborhood and the coefficient matrix of the given shoeprint image is also considered to develop a powerful feature descriptor.
The descriptor is based on the Fourier-Mellin Transform similar to the approaches previously introduced, thus a combination is possible.
\par
Voting scores and feature-learning are machine learning techniques, but deep learning \cite{lecun2015deep} was not considered either.
Rida et al. \cite{rida2019forensic} discuss the possibility of using Convolutional Neural Network (CNN) for end-to-end shoeprint recognition and identification introduced in \cite{lecun1998gradient}.
Kong et al. \cite{kong2017cross}, \cite{kong2019cross} already published an approach for taking advantage on CNNs.
However, instead of using the CNN to identify the given shoeprint, they extract mid-level features of the network to use them as feature descriptor.
Even though there are publications for natural image enhancement using deep learning \cite{gharbi2017deep}, \cite{chen2018deep}, it has to be investigated, how their tools can be used for forensic shoeprint image.
\par
In all approaches proposed in this thesis, the geometrical location of the extracted features is disregarded, even though it is an important information, while identifying shoeprint images.
Despite that several samples depict only a partial image, as soon as there are at least two different patterns or a recognizable outline part of the shoe is visible, assumptions are possible about the geometrical location of the given pattern of the shoeprint.
Li et al. \cite{li2015secondary} propose a subdivision method where four regions of a shoeprint are distinguished, and processed according to their position.
These are cap, sole, arch and heel.
Additionally, the overall silhouette of a shoeprint also contains information.
Wang et al. \cite{wang2014automatic} developed a shoeprint contour model for shoeprint retrieval on high-quality samples, which helps to make their approach more robust, since it is able to correct distortions or noise on the input shoeprint images.
\par
Lastly, Randomly Acquired Features (RAC) of shoeprints were already mentioned before, but were not considered during feature extraction.
As Rida et al. \cite{rida2019forensic} states, in many dataset only a limited amount of shoeprint images are available for one shoe outsole pattern which is also the case in the FID-300 database.
The majority of the dataset consists of single shoeprint images, and there are no more than six shoeprint images from the same shoesole in exceptional cases. 
However, assuming that the demand for wider dataset increases with active research, considering RAC features of the input provides valuable additional information about the given shoeprint image.
Shor et al. \cite{shor2018inherent} introduce a technique to distinguish between the shoeprint images of the same shoesole based on the obtained RAC features.
Damary et al. \cite{damary2018dependence} published a study about the estimated properties of a RAC, according to its location on the shoeprint.
This way if a RAC is identified, assumptions about the location of the neighboring pattern are possible.
Similar to basic structure description for shoeprints \cite{tang2010footwear}, a technique for RAC description was also developed \cite{speir2016quantifying}, thus incorporating RAC models into shoeprint descriptors is already possible.
\par
Considering the general research done on the topic of shoeprint enhancement, there are general issues that have to be solved in the future.
The current practice of using several datasets has several drawbacks.
The proposed results are not meaningful and consequential, since the used datasets are not public in many cases, and because the results are not comparable.
Furthermore, the real-life performance and usefulness of the published methods is unknown, because they are often tested on synthetic and on handcrafted data.
Rida et al. \cite{rida2019forensic} recently proposed a survey about the research done in the last 20 years, and made an overview about the accuracy of the published methods.
Several authors claimed to have above 99\% \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{almaadeed2015partial}, or exactly 100\% accuracy \cite{gueham2007automatic} on their testing datasets.
However, the algorithms were tested on different datasets, thus an absolute comparison between their performance is not possible.

% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

\backmatter

% Use an optional list of figures.
%\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
%\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
%\printindex

% Add a glossary.
%\printglossaries

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{intro}

\begin{appendices}
		\chapter{Appendix Dataset}
		\label{AppA}
		This section contains the images selected for testing.
		The samples were chosen from FID-300, and are representative for the database.
No restriction were made while selecting the images, low quality images are also part of the experimental setting.
		In the first column, the original images are shown, in the second and third columns the results using 20 classes for classification of the gradient difference image is visible, whereas in the fourth and fifth column the output with 25 classes is displayed.
		The results using the mean noise model are in columns two and four, the output using the centroids are in columns three and five.
\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00009_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00017.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00017_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00020.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00020_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00021.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00021_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00025.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00025_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00066.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00066_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00171.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00171_filtered.jpg}
  \end{subfigure}
\caption{}
\label{appA:171}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00174.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00174_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00178.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00178_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00182.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00182_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00197.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00197_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00204.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00204_filtered.jpg}
  \end{subfigure}
\caption{}
  \label{appA:204}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00217.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00217_filtered.jpg}
  \end{subfigure}
\caption{}
\label{appA:217}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00223.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00223_filtered.jpg}
  \end{subfigure}
\caption{}
 \label{appA:223}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00232.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00232_filtered.jpg}
  \end{subfigure}
\caption{}
\label{appA:232}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00233.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00233_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00235.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00235_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00239.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00239_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00241.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00241_filtered.jpg}
  \end{subfigure}
	\caption{}
  \label{appA:241}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00250.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00250_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\chapter{Appendix Results}
\label{AppB}
		This section contains the quantitative results of the Shoeprint Enhancement Algorithm with Non-Local Means.
		The testing dataset is presented in \ref{AppA}.
		The accumulated results are presented and discussed in Chapter 4 \ref{results}.
		The four tables, see \ref{tab:25Mean}, \ref{tab:25Cent} and \ref{tab:20Mean}, \ref{tab:20Cent}, present the matching scores of the different settings separately and are given to enlighten the reproducibility of the algorithm.
		Furthermore, to create a baseline, the original samples were matched to the reference images \ref{tab:BL}.

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|cc|cc|cc}
& \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
\hline
1 & 12 & 13 & 9 & 4 & 1 & 1 \\
2 & 4 & 5 & 1 & 3 & 2 & 2 \\
3 & 5 & 7 & 2 & 1 & 4 & 4 \\
4 & 5 & 6 & 1 & 1 & 5 & 2 \\
5 & 6 & 1 & 1 & 1 & 5 & 4 \\
6 & 6 & 6 & 4 & 6 & 6 & 6 \\
8 & 3 & 4 & 4 & 3 & 8 & 8 \\
9 & 2 & 1 & 7 & 4 & 7 & 7 \\
10 & 9 & 10 & 12 & 9 & 13 & 12 \\
11 & 2 & 1 & 12 & 13 & 11 & 11 \\
16 & 6 & 5 & 1 & 1 & 9 & 6 \\
17 & 1 & 1 & 6 & 5 & 1 & 1 \\
18 & 13 & 11 & 12 & 7 & 12 & 12 \\
20 & 8 & 11 & 6 & 4 & 7 & 3 \\
\hline
Score & 5.86 & 5.86 & 5.57 & 4.43 & 6.50 & 6.64 \\
\end{tabular}
\captionof{table}{Baseline results matching the original images with the unprocessed Ground Truth images}
 \label{tab:BL} 
\end{minipage}


\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|cc|cc|cc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
\hline
1 & 1 & 1 & 2 & 6 & 4 & 5 \\
2 & 1 & 1 & 3 & 6 & 4 & 4 \\
3 & 1 & 2 & 1 & 1 & 3 & 4 \\
4 & 1 & 1 & 4 & 8 & 1 & 2 \\
5 & 4 & 5 & 1 & 1 & 4 & 6 \\
6 & 7 & 5 & 5 & 5 & 8 & 8 \\
8 & 3 & 3 & 13 & 12 & 6 & 4 \\
9 & 7 & 5 & 3 & 4 & 1 & 3 \\
10 & 5 & 3 & 2 & 4 & 3 & 5 \\
11 & 2 & 3 & 3 & 5 & 10 & 12 \\
16 & 2 & 2 & 6 & 5 & 5 & 4 \\
17 & 4 & 4 & 8 & 11 & 1 & 1 \\
18 & 11 & 12 & 4 & 5 & 2 & 1 \\
20 & 5 & 6 & 4 & 5 & 3 & 3 \\
\hline
Score & 3.86 & 3.79 & 4.21 & 5.57 & 3.93 & 4.43 \\
\end{tabular}

\captionof{table}{Results using 25 classes and the average noise model}
 \label{tab:25Mean} 
\end{minipage}


\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|cc|cc|cc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
\hline
1 & 2 & 3 & 1 & 2 & 3 & 4 \\
2 & 1 & 3 & 3 & 4 & 6 & 8 \\
3 & 1 & 2 & 8 & 6 & 2 & 5 \\
4 & 1 & 1 & 2 & 5 & 3 & 5 \\
5 & 1 & 1 & 1 & 1 & 4 & 5 \\
6 & 5 & 4 & 6 & 9 & 7 & 7 \\
8 & 3 & 4 & 12 & 13 & 8 & 9 \\
9 & 7 & 5 & 3 & 4 & 1 & 3 \\
10 & 6 & 5 & 1 & 1 & 4 & 4 \\
11 & 3 & 2 & 9 & 6 & 11 & 11 \\
16 & 4 & 4 & 2 & 8 & 6 & 7 \\
17 & 1 & 1 & 4 & 8 & 1 & 2 \\
18 & 9 & 9 & 5 & 1 & 4 & 1 \\
20 & 2 & 1 & 3 & 4 & 2 & 4 \\
\hline
Score & 3.29 & 3.21 & 4.29 & 5.14 & 4.43 & 5.36 \\
\end{tabular}
\captionof{table}{Results using 25 classes and the centroids noise model}
 \label{tab:25Cent} 
\end{minipage}

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|cc|cc|cc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
\hline
1 & 1 & 1 & 2 & 4 & 3 & 4 \\
2 & 3 & 3 & 6 & 3 & 2 & 3 \\
3 & 1 & 2 & 1 & 1 & 4 & 5 \\
4 & 2 & 5 & 6 & 7 & 1 & 1 \\
5 & 2 & 3 & 1 & 1 & 2 & 4 \\
6 & 4 & 8 & 10 & 12 & 6 & 8 \\
8 & 5 & 5 & 11 & 12 & 3 & 4 \\
9 & 7 & 5 & 3 & 4 & 1 & 3 \\
10 & 7 & 4 & 1 & 2 & 1 & 4 \\
11 & 2 & 1 & 6 & 6 & 3 & 2 \\
16 & 10 & 9 & 3 & 7 & 4 & 5 \\
17 & 2 & 2 & 11 & 6 & 1 & 1 \\
18 & 12 & 12 & 4 & 3 & 9 & 5 \\
20 & 4 & 4 & 4 & 5 & 2 & 2 \\
\hline
Score & 4.43 & 4.57 & 4.93 & 5.21 & 3.00 & 3.64 \\
\end{tabular}

\captionof{table}{Results using 20 classes and the average noise model}
 \label{tab:20Mean} 
\end{minipage}

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{c|cc|cc|cc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
\hline
1 & 1 & 1 & 1 & 3 & 1 & 2 \\
2 & 1 & 1 & 2 & 2 & 5 & 8 \\
3 & 1 & 2 & 4 & 8 & 5 & 5 \\
4 & 1 & 1 & 3 & 8 & 2 & 2 \\
5 & 3 & 3 & 1 & 1 & 4 & 4 \\
6 & 2 & 2 & 8 & 12 & 5 & 5 \\
8 & 6 & 5 & 9 & 12 & 7 & 6 \\
9 & 7 & 5 & 3 & 4 & 1 & 3 \\
10 & 4 & 2 & 8 & 5 & 1 & 1 \\
11 & 6 & 2 & 6 & 8 & 11 & 10 \\
16 & 1 & 1 & 5 & 6 & 1 & 2 \\
17 & 2 & 3 & 3 & 4 & 1 & 6 \\
18 & 9 & 11 & 7 & 4 & 3 & 1 \\
20 & 2 & 4 & 2 & 3 & 5 & 8 \\
\hline
Score & 3.29 & 3.07 & 4.43 & 5.71 & 3.71 & 4.50 \\
\end{tabular}

\captionof{table}{Results using 20 classes and the centroids noise model}
 \label{tab:20Cent} 
\end{minipage}


\end{appendices}

\end{document}